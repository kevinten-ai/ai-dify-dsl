#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dify Workflow Manager v2.0
===========================

åŠŸèƒ½: å®Œæ•´çš„Difyå·¥ä½œæµç®¡ç†å·¥å…·ï¼Œæ”¯æŒæ‹†åˆ†ã€ä¿®æ”¹ã€é‡å»ºã€å¯¹æ¯”çš„å®Œæ•´å·¥ä½œæµ
ä½œè€…: AI Assistant
æ—¥æœŸ: 2024-01-15

ä¸»è¦åŠŸèƒ½:
1. split  - æ‹†åˆ†YAMLä¸ºç‹¬ç«‹æ–‡ä»¶ï¼ˆä¿ç•™åŸå§‹æ ¼å¼ï¼‰
2. rebuild - é‡å»ºYAMLæ–‡ä»¶
3. compare - å¯¹æ¯”YAMLå·®å¼‚
4. validate - éªŒè¯ä¸€è‡´æ€§

ä½¿ç”¨æ–¹æ³•:
    python dify_workflow_manager_v2.py split "AI Code Review-V4.1.yml"
    python dify_workflow_manager_v2.py rebuild "parsed_workflow_V4.1_20240115_143022"
    python dify_workflow_manager_v2.py compare "original.yml" "rebuilt.yml"
"""

import os
import sys
import json
import re
import difflib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import logging
import jsonschema
from jsonschema import ValidationError
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å°è¯•å¯¼å…¥ruamel.yamlï¼ˆä¿ç•™æ ¼å¼ï¼‰,å¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°pyyaml
try:
    from ruamel.yaml import YAML
    YAML_AVAILABLE = True
except ImportError:
    import yaml
    YAML_AVAILABLE = False
    print("è­¦å‘Š: ruamel.yamlæœªå®‰è£…ï¼Œå°†ä½¿ç”¨pyyamlï¼ˆå¯èƒ½ä¸¢å¤±æ ¼å¼ä¿¡æ¯ï¼‰")
    print("å»ºè®®å®‰è£…: pip install ruamel.yaml")


class ErrorHandler:
    """é”™è¯¯å¤„ç†å™¨ - æä¾›å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶"""

    def __init__(self, max_retries: int = 3, enable_recovery: bool = True):
        self.max_retries = max_retries
        self.enable_recovery = enable_recovery
        self.error_log = []
        self.logger = logging.getLogger(__name__)

    def handle_operation(self, operation_func, operation_name: str, *args, **kwargs):
        """å¤„ç†æ“ä½œï¼ŒåŒ…å«é‡è¯•å’Œé”™è¯¯æ¢å¤"""
        last_exception = None

        for attempt in range(self.max_retries):
            try:
                result = operation_func(*args, **kwargs)
                if attempt > 0:
                    self.logger.info(f"{operation_name} åœ¨ç¬¬ {attempt + 1} æ¬¡å°è¯•åæˆåŠŸ")
                return result
            except Exception as e:
                last_exception = e
                self.error_log.append({
                    'operation': operation_name,
                    'attempt': attempt + 1,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })

                if attempt < self.max_retries - 1:
                    self.logger.warning(f"{operation_name} ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œå°†é‡è¯•: {e}")
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æŒ‡æ•°é€€é¿ç­‰ç­–ç•¥
                else:
                    self.logger.error(f"{operation_name} åœ¨ {self.max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {e}")

        # å¦‚æœå¯ç”¨äº†æ¢å¤æœºåˆ¶ï¼Œå°è¯•æ¢å¤æ“ä½œ
        if self.enable_recovery:
            return self._attempt_recovery(operation_name, last_exception, *args, **kwargs)

        raise last_exception

    def _attempt_recovery(self, operation_name: str, exception: Exception, *args, **kwargs):
        """å°è¯•æ¢å¤å¤±è´¥çš„æ“ä½œ"""
        self.logger.info(f"å°è¯•æ¢å¤æ“ä½œ: {operation_name}")

        # æ ¹æ®æ“ä½œç±»å‹å°è¯•ä¸åŒçš„æ¢å¤ç­–ç•¥
        if 'split' in operation_name.lower():
            return self._recover_split_operation(operation_name, exception, *args, **kwargs)
        elif 'rebuild' in operation_name.lower():
            return self._recover_rebuild_operation(operation_name, exception, *args, **kwargs)
        elif 'validate' in operation_name.lower():
            return self._recover_validation_operation(operation_name, exception, *args, **kwargs)

        # é»˜è®¤æƒ…å†µä¸‹é‡æ–°æŠ›å‡ºå¼‚å¸¸
        raise exception

    def _recover_split_operation(self, operation_name: str, exception: Exception, *args, **kwargs):
        """æ¢å¤æ‹†åˆ†æ“ä½œ"""
        try:
            # å°è¯•åˆ›å»ºåŸºæœ¬çš„è¾“å‡ºç»“æ„ï¼Œå³ä½¿æŸäº›èŠ‚ç‚¹å¤±è´¥
            if len(args) > 0 and hasattr(args[0], '_create_output_structure'):
                splitter = args[0]
                splitter._create_output_structure()
                self.logger.info("å·²åˆ›å»ºåŸºæœ¬çš„è¾“å‡ºç»“æ„ç”¨äºæ¢å¤")
                return True
        except Exception as recovery_error:
            self.logger.error(f"æ‹†åˆ†æ“ä½œæ¢å¤å¤±è´¥: {recovery_error}")

        raise exception

    def _recover_rebuild_operation(self, operation_name: str, exception: Exception, *args, **kwargs):
        """æ¢å¤é‡å»ºæ“ä½œ"""
        try:
            # å°è¯•é‡å»ºéƒ¨åˆ†æˆåŠŸçš„èŠ‚ç‚¹
            if len(args) > 0 and hasattr(args[0], '_rebuild_yaml_structure'):
                rebuilder = args[0]
                # å°è¯•åªé‡å»ºæˆåŠŸçš„éƒ¨åˆ†
                partial_structure = rebuilder._rebuild_yaml_structure(rebuilder._load_metadata())
                if partial_structure:
                    self.logger.info("å·²é‡å»ºéƒ¨åˆ†å·¥ä½œæµç»“æ„")
                    return partial_structure
        except Exception as recovery_error:
            self.logger.error(f"é‡å»ºæ“ä½œæ¢å¤å¤±è´¥: {recovery_error}")

        raise exception

    def _recover_validation_operation(self, operation_name: str, exception: Exception, *args, **kwargs):
        """æ¢å¤éªŒè¯æ“ä½œ"""
        # å¯¹äºéªŒè¯æ“ä½œï¼Œé€šå¸¸ä¸éœ€è¦æ¢å¤ï¼Œç›´æ¥è¿”å›éªŒè¯å¤±è´¥çš„ç»“æœ
        return False, [str(exception)]

    def generate_error_report(self, output_path: str = None) -> str:
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        if not self.error_log:
            return "æ— é”™è¯¯è®°å½•"

        report_lines = [
            "# å·¥ä½œæµå¤„ç†é”™è¯¯æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
            f"é”™è¯¯æ•°é‡: {len(self.error_log)}",
            "",
            "## é”™è¯¯è¯¦æƒ…",
            ""
        ]

        for i, error in enumerate(self.error_log, 1):
            report_lines.extend([
                f"### é”™è¯¯ {i}",
                f"- æ“ä½œ: {error['operation']}",
                f"- å°è¯•æ¬¡æ•°: {error['attempt']}",
                f"- æ—¶é—´: {error['timestamp']}",
                f"- é”™è¯¯ä¿¡æ¯: {error['error']}",
                ""
            ])

        report_content = "\n".join(report_lines)

        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                self.logger.info(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜é”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")

        return report_content

    def clear_errors(self):
        """æ¸…é™¤é”™è¯¯æ—¥å¿—"""
        self.error_log.clear()


class DSLValidator:
    """Dify DSLç»“æ„éªŒè¯å™¨"""

    # Dify DSL JSON Schemaå®šä¹‰
    DIFY_SCHEMA = {
        "type": "object",
        "required": ["app", "kind", "version", "workflow"],
        "properties": {
            "app": {
                "type": "object",
                "required": ["name", "mode"],
                "properties": {
                    "name": {"type": "string"},
                    "description": {"type": "string"},
                    "icon": {"type": "string"},
                    "icon_background": {"type": "string"},
                    "mode": {"type": "string", "enum": ["workflow", "chat", "completion"]}
                }
            },
            "kind": {"type": "string", "enum": ["app"]},
            "version": {"type": "string"},
            "dependencies": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "current_identifier": {"type": ["string", "null"]},
                        "type": {"type": "string", "enum": ["marketplace", "package"]},
                        "value": {"type": "object"}
                    }
                }
            },
            "workflow": {
                "type": "object",
                "required": ["graph"],
                "properties": {
                    "conversation_variables": {"type": "array"},
                    "environment_variables": {"type": "array"},
                    "features": {"type": "object"},
                    "graph": {
                        "type": "object",
                        "required": ["nodes", "edges"],
                        "properties": {
                            "nodes": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "required": ["id", "data"],
                                    "properties": {
                                        "id": {"type": "string"},
                                        "data": {
                                            "type": "object",
                                            "required": ["type", "title"],
                                            "properties": {
                                                "type": {"type": "string"},
                                                "title": {"type": "string"},
                                                "position": {"type": "object"}
                                            }
                                        },
                                        "position": {"type": "object"},
                                        "height": {"type": "number"},
                                        "width": {"type": "number"},
                                        "selected": {"type": "boolean"},
                                        "sourcePosition": {"type": "string"},
                                        "targetPosition": {"type": "string"},
                                        "type": {"type": "string"}
                                    }
                                }
                            },
                            "edges": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "required": ["id", "source", "target"],
                                    "properties": {
                                        "id": {"type": "string"},
                                        "source": {"type": "string"},
                                        "target": {"type": "string"},
                                        "sourceHandle": {"type": "string"},
                                        "targetHandle": {"type": "string"},
                                        "data": {"type": "object"},
                                        "type": {"type": "string"}
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    # èŠ‚ç‚¹ç±»å‹éªŒè¯è§„åˆ™
    NODE_TYPE_RULES = {
        'start': {'required_fields': ['type', 'title'], 'allowed_connections': ['outgoing']},
        'end': {'required_fields': ['type', 'title'], 'allowed_connections': ['incoming']},
        'llm': {
            'required_fields': ['type', 'title', 'model_provider', 'model_name'],
            'optional_fields': ['prompt_template', 'model_parameters', 'context'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'code': {
            'required_fields': ['type', 'title', 'code_language'],
            'optional_fields': ['code', 'variables', 'outputs'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'agent': {
            'required_fields': ['type', 'title', 'agent_strategy_provider_name', 'agent_strategy_name'],
            'optional_fields': ['agent_parameters', 'prompt_template'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'tool': {
            'required_fields': ['type', 'title', 'provider', 'tool_name'],
            'optional_fields': ['tool_parameters', 'tool_credential'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'http_request': {
            'required_fields': ['type', 'title', 'method', 'url'],
            'optional_fields': ['headers', 'params', 'body', 'authorization', 'timeout'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'if_else': {
            'required_fields': ['type', 'title', 'conditions', 'logical_operator'],
            'optional_fields': ['variable_selectors'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'iteration': {
            'required_fields': ['type', 'title', 'iterator_selector', 'start_node_id'],
            'optional_fields': ['output_selector', 'output_type'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'template_transform': {
            'required_fields': ['type', 'title', 'template'],
            'optional_fields': ['variable_selectors'],
            'allowed_connections': ['incoming', 'outgoing']
        },
        'knowledge_retrieval': {
            'required_fields': ['type', 'title', 'dataset_ids', 'retrieval_mode'],
            'optional_fields': ['query_variable_selector', 'all_datasets'],
            'allowed_connections': ['incoming', 'outgoing']
        }
    }

    @staticmethod
    def validate_dsl_structure(workflow_data: Dict) -> Tuple[bool, List[str]]:
        """éªŒè¯DSLæ•´ä½“ç»“æ„"""
        errors = []

        try:
            # ä½¿ç”¨JSON SchemaéªŒè¯åŸºç¡€ç»“æ„
            jsonschema.validate(instance=workflow_data, schema=DSLValidator.DIFY_SCHEMA)
        except ValidationError as e:
            errors.append(f"DSLç»“æ„éªŒè¯å¤±è´¥: {e.message}")
            return False, errors

        # éªŒè¯å·¥ä½œæµå›¾ç»“æ„
        graph_errors = DSLValidator._validate_graph_structure(workflow_data)
        errors.extend(graph_errors)

        # éªŒè¯èŠ‚ç‚¹è¿æ¥å…³ç³»
        connection_errors = DSLValidator._validate_node_connections(workflow_data)
        errors.extend(connection_errors)

        # éªŒè¯èŠ‚ç‚¹é…ç½®
        node_errors = DSLValidator._validate_node_configurations(workflow_data)
        errors.extend(node_errors)

        return len(errors) == 0, errors

    @staticmethod
    def _validate_graph_structure(workflow_data: Dict) -> List[str]:
        """éªŒè¯å›¾ç»“æ„"""
        errors = []

        graph = workflow_data.get('workflow', {}).get('graph', {})
        nodes = graph.get('nodes', [])
        edges = graph.get('edges', [])

        # æ£€æŸ¥æ˜¯å¦æœ‰èŠ‚ç‚¹
        if not nodes:
            errors.append("å·¥ä½œæµå›¾ä¸­æ²¡æœ‰èŠ‚ç‚¹")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
        node_types = [node.get('data', {}).get('type') for node in nodes]
        if 'start' not in node_types:
            errors.append("å·¥ä½œæµç¼ºå°‘å¼€å§‹èŠ‚ç‚¹")
        if 'end' not in node_types:
            errors.append("å·¥ä½œæµç¼ºå°‘ç»“æŸèŠ‚ç‚¹")

        # æ£€æŸ¥èŠ‚ç‚¹IDå”¯ä¸€æ€§
        node_ids = [node.get('id') for node in nodes]
        if len(node_ids) != len(set(node_ids)):
            errors.append("èŠ‚ç‚¹IDä¸å”¯ä¸€")

        # æ£€æŸ¥è¾¹å¼•ç”¨æœ‰æ•ˆæ€§
        node_id_set = set(node_ids)
        for edge in edges:
            source_id = edge.get('source')
            target_id = edge.get('target')
            if source_id not in node_id_set:
                errors.append(f"è¾¹ {edge.get('id')} å¼•ç”¨äº†ä¸å­˜åœ¨çš„æºèŠ‚ç‚¹ {source_id}")
            if target_id not in node_id_set:
                errors.append(f"è¾¹ {edge.get('id')} å¼•ç”¨äº†ä¸å­˜åœ¨çš„ç›®æ ‡èŠ‚ç‚¹ {target_id}")

        return errors

    @staticmethod
    def _validate_node_connections(workflow_data: Dict) -> List[str]:
        """éªŒè¯èŠ‚ç‚¹è¿æ¥å…³ç³»"""
        errors = []

        graph = workflow_data.get('workflow', {}).get('graph', {})
        nodes = graph.get('nodes', [])
        edges = graph.get('edges', [])

        # æ„å»ºèŠ‚ç‚¹è¿æ¥å›¾
        incoming_connections = {}
        outgoing_connections = {}

        for node in nodes:
            node_id = node.get('id')
            incoming_connections[node_id] = []
            outgoing_connections[node_id] = []

        for edge in edges:
            source_id = edge.get('source')
            target_id = edge.get('target')
            outgoing_connections[source_id].append(target_id)
            incoming_connections[target_id].append(source_id)

        # éªŒè¯è¿æ¥è§„åˆ™
        for node in nodes:
            node_id = node.get('id')
            node_type = node.get('data', {}).get('type')

            # å¼€å§‹èŠ‚ç‚¹ä¸èƒ½æœ‰å…¥è¾¹
            if node_type == 'start' and incoming_connections[node_id]:
                errors.append(f"å¼€å§‹èŠ‚ç‚¹ {node_id} ä¸èƒ½æœ‰å…¥è¾¹")

            # ç»“æŸèŠ‚ç‚¹ä¸èƒ½æœ‰å‡ºè¾¹
            if node_type == 'end' and outgoing_connections[node_id]:
                errors.append(f"ç»“æŸèŠ‚ç‚¹ {node_id} ä¸èƒ½æœ‰å‡ºè¾¹")

            # æ£€æŸ¥å¾ªç¯ä¾èµ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if DSLValidator._has_circular_dependency(node_id, outgoing_connections):
                errors.append(f"èŠ‚ç‚¹ {node_id} å­˜åœ¨å¾ªç¯ä¾èµ–")

        return errors

    @staticmethod
    def _validate_node_configurations(workflow_data: Dict) -> List[str]:
        """éªŒè¯èŠ‚ç‚¹é…ç½®"""
        errors = []

        nodes = workflow_data.get('workflow', {}).get('graph', {}).get('nodes', [])

        for node in nodes:
            node_data = node.get('data', {})
            node_type = node_data.get('type')
            node_id = node.get('id')

            # è·å–èŠ‚ç‚¹éªŒè¯è§„åˆ™
            rules = DSLValidator.NODE_TYPE_RULES.get(node_type, {})

            if not rules:
                # å¯¹äºæœªçŸ¥èŠ‚ç‚¹ç±»å‹ï¼Œç»™å‡ºè­¦å‘Šä½†ä¸æŠ¥é”™
                continue

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = rules.get('required_fields', [])
            for field in required_fields:
                if field not in node_data:
                    errors.append(f"èŠ‚ç‚¹ {node_id} ({node_type}) ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

            # éªŒè¯ç‰¹å®šå­—æ®µ
            if node_type == 'llm':
                errors.extend(DSLValidator._validate_llm_node(node_data, node_id))
            elif node_type == 'agent':
                errors.extend(DSLValidator._validate_agent_node(node_data, node_id))
            elif node_type == 'http_request':
                errors.extend(DSLValidator._validate_http_node(node_data, node_id))

        return errors

    @staticmethod
    def _validate_llm_node(node_data: Dict, node_id: str) -> List[str]:
        """éªŒè¯LLMèŠ‚ç‚¹é…ç½®"""
        errors = []

        # æ£€æŸ¥æ¨¡å‹æä¾›å•†
        model_provider = node_data.get('model_provider')
        if model_provider and model_provider not in ['openai', 'anthropic', 'google', 'azure_openai']:
            errors.append(f"LLMèŠ‚ç‚¹ {node_id} ä½¿ç”¨äº†ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {model_provider}")

        # æ£€æŸ¥æç¤ºè¯æ¨¡æ¿
        prompt_template = node_data.get('prompt_template', [])
        if not isinstance(prompt_template, list):
            errors.append(f"LLMèŠ‚ç‚¹ {node_id} çš„æç¤ºè¯æ¨¡æ¿å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, prompt in enumerate(prompt_template):
                if not isinstance(prompt, dict) or 'role' not in prompt or 'text' not in prompt:
                    errors.append(f"LLMèŠ‚ç‚¹ {node_id} çš„æç¤ºè¯ {i} æ ¼å¼ä¸æ­£ç¡®")

        return errors

    @staticmethod
    def _validate_agent_node(node_data: Dict, node_id: str) -> List[str]:
        """éªŒè¯AgentèŠ‚ç‚¹é…ç½®"""
        errors = []

        # æ£€æŸ¥ç­–ç•¥é…ç½®
        strategy_provider = node_data.get('agent_strategy_provider_name')
        strategy_name = node_data.get('agent_strategy_name')

        if not strategy_provider:
            errors.append(f"AgentèŠ‚ç‚¹ {node_id} ç¼ºå°‘ç­–ç•¥æä¾›å•†é…ç½®")
        if not strategy_name:
            errors.append(f"AgentèŠ‚ç‚¹ {node_id} ç¼ºå°‘ç­–ç•¥åç§°é…ç½®")

        # æ£€æŸ¥Agentå‚æ•°
        agent_parameters = node_data.get('agent_parameters', {})
        if agent_parameters and not isinstance(agent_parameters, dict):
            errors.append(f"AgentèŠ‚ç‚¹ {node_id} çš„å‚æ•°æ ¼å¼ä¸æ­£ç¡®")

        return errors

    @staticmethod
    def _validate_http_node(node_data: Dict, node_id: str) -> List[str]:
        """éªŒè¯HTTPèŠ‚ç‚¹é…ç½®"""
        errors = []

        # æ£€æŸ¥URLæ ¼å¼
        url = node_data.get('url', '')
        if url and not (url.startswith('http://') or url.startswith('https://')):
            errors.append(f"HTTPèŠ‚ç‚¹ {node_id} çš„URLæ ¼å¼ä¸æ­£ç¡®: {url}")

        # æ£€æŸ¥æ–¹æ³•
        method = node_data.get('method', '').upper()
        valid_methods = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'HEAD', 'OPTIONS']
        if method and method not in valid_methods:
            errors.append(f"HTTPèŠ‚ç‚¹ {node_id} ä½¿ç”¨äº†ä¸æ”¯æŒçš„æ–¹æ³•: {method}")

        # æ£€æŸ¥è¶…æ—¶æ—¶é—´
        timeout = node_data.get('timeout', 60)
        if not isinstance(timeout, (int, float)) or timeout <= 0:
            errors.append(f"HTTPèŠ‚ç‚¹ {node_id} çš„è¶…æ—¶æ—¶é—´å¿…é¡»æ˜¯æ­£æ•°")

        return errors

    @staticmethod
    def _has_circular_dependency(node_id: str, outgoing_connections: Dict, visited: set = None, path: set = None) -> bool:
        """æ£€æµ‹å¾ªç¯ä¾èµ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if visited is None:
            visited = set()
        if path is None:
            path = set()

        if node_id in path:
            return True

        if node_id in visited:
            return False

        visited.add(node_id)
        path.add(node_id)

        for neighbor in outgoing_connections.get(node_id, []):
            if DSLValidator._has_circular_dependency(neighbor, outgoing_connections, visited, path):
                return True

        path.remove(node_id)
        return False


class WorkflowVersionManager:
    """å·¥ä½œæµç‰ˆæœ¬ç®¡ç†å™¨ - å¢å¼ºç‰ˆï¼Œæ”¯æŒç‰ˆæœ¬å…¼å®¹æ€§å’Œè¿ç§»"""

    # Dify DSLç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ
    VERSION_COMPATIBILITY = {
        "0.1.0": {"compatible_versions": ["0.1.0"], "features": ["basic_workflow"]},
        "0.1.1": {"compatible_versions": ["0.1.0", "0.1.1"], "features": ["basic_workflow", "conversation_variables"]},
        "0.1.2": {"compatible_versions": ["0.1.0", "0.1.1", "0.1.2"], "features": ["basic_workflow", "conversation_variables", "environment_variables"]},
        "0.1.3": {"compatible_versions": ["0.1.0", "0.1.1", "0.1.2", "0.1.3"], "features": ["basic_workflow", "conversation_variables", "environment_variables", "features_config"]},
        "0.1.4": {"compatible_versions": ["0.1.0", "0.1.1", "0.1.2", "0.1.3", "0.1.4"], "features": ["basic_workflow", "conversation_variables", "environment_variables", "features_config", "advanced_tools"]},
        "0.1.5": {"compatible_versions": ["0.1.0", "0.1.1", "0.1.2", "0.1.3", "0.1.4", "0.1.5"], "features": ["basic_workflow", "conversation_variables", "environment_variables", "features_config", "advanced_tools", "mcp_support"]}
    }

    # ç‰ˆæœ¬è¿ç§»è§„åˆ™
    MIGRATION_RULES = {
        "0.1.0_to_0.1.1": {
            "add_fields": {
                "workflow.conversation_variables": []
            }
        },
        "0.1.1_to_0.1.2": {
            "add_fields": {
                "workflow.environment_variables": []
            }
        },
        "0.1.2_to_0.1.3": {
            "add_fields": {
                "workflow.features": {}
            }
        },
        "0.1.3_to_0.1.4": {
            "add_fields": {
                "dependencies": []
            }
        },
        "0.1.4_to_0.1.5": {
            "add_fields": {
                "workflow.features.mcp_enabled": False
            }
        }
    }

    @staticmethod
    def extract_version_from_filename(filename: str) -> str:
        """ä»æ–‡ä»¶åæå–ç‰ˆæœ¬å· - å¢å¼ºç‰ˆ"""
        # åŒ¹é…æ¨¡å¼å¦‚: "AI Code Review-V4.1.yml" -> "V4.1"
        patterns = [
            r'-[Vv](\d+\.\d+)',  # -V4.1 æˆ– -v4.1
            r'[Vv](\d+\.\d+)',   # V4.1 æˆ– v4.1
            r'-(\d+\.\d+)',      # -4.1
            r'[_-]v?(\d+)_(\d+)', # v4_1 æˆ– -v4_1
            r'version[_-]?(\d+\.\d+)', # version4.1
        ]

        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                if len(match.groups()) == 2:
                    # å¤„ç† v4_1 æ ¼å¼
                    major, minor = match.groups()
                    return f"V{major}.{minor}"
                else:
                    return f"V{match.group(1)}"

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œä½¿ç”¨é»˜è®¤
        return "V1.0"

    @staticmethod
    def parse_dsl_version(workflow_data: Dict) -> str:
        """è§£æDSLç‰ˆæœ¬å·"""
        version = workflow_data.get('version', '0.1.0')
        if isinstance(version, str) and version.startswith('0.'):
            return version
        return '0.1.0'

    @staticmethod
    def check_version_compatibility(source_version: str, target_version: str) -> Tuple[bool, str]:
        """æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§"""
        if source_version not in WorkflowVersionManager.VERSION_COMPATIBILITY:
            return False, f"ä¸æ”¯æŒçš„æºç‰ˆæœ¬: {source_version}"

        if target_version not in WorkflowVersionManager.VERSION_COMPATIBILITY:
            return False, f"ä¸æ”¯æŒçš„ç›®æ ‡ç‰ˆæœ¬: {target_version}"

        source_info = WorkflowVersionManager.VERSION_COMPATIBILITY[source_version]
        compatible_versions = source_info["compatible_versions"]

        if target_version in compatible_versions:
            return True, "ç‰ˆæœ¬å…¼å®¹"
        else:
            return False, f"ç‰ˆæœ¬ {source_version} ä¸ {target_version} ä¸å…¼å®¹"

    @staticmethod
    def migrate_workflow(workflow_data: Dict, target_version: str) -> Tuple[bool, Dict, List[str]]:
        """è¿ç§»å·¥ä½œæµåˆ°ç›®æ ‡ç‰ˆæœ¬"""
        current_version = WorkflowVersionManager.parse_dsl_version(workflow_data)

        if current_version == target_version:
            return True, workflow_data, ["å·¥ä½œæµå·²æ˜¯ç›®æ ‡ç‰ˆæœ¬"]

        # æ£€æŸ¥å…¼å®¹æ€§
        is_compatible, message = WorkflowVersionManager.check_version_compatibility(current_version, target_version)
        if not is_compatible:
            return False, workflow_data, [f"ç‰ˆæœ¬è¿ç§»å¤±è´¥: {message}"]

        # æ‰§è¡Œè¿ç§»
        migrated_data = workflow_data.copy()
        migration_log = []

        # æŒ‰é¡ºåºæ‰§è¡Œè¿ç§»æ­¥éª¤
        version_steps = WorkflowVersionManager._get_migration_path(current_version, target_version)

        for step in version_steps:
            rule_key = f"{step['from']}_to_{step['to']}"
            if rule_key in WorkflowVersionManager.MIGRATION_RULES:
                rule = WorkflowVersionManager.MIGRATION_RULES[rule_key]
                migrated_data, step_log = WorkflowVersionManager._apply_migration_rule(migrated_data, rule)
                migration_log.extend(step_log)

        # æ›´æ–°ç‰ˆæœ¬å·
        migrated_data['version'] = target_version
        migration_log.append(f"æˆåŠŸè¿ç§»åˆ°ç‰ˆæœ¬ {target_version}")

        return True, migrated_data, migration_log

    @staticmethod
    def _get_migration_path(from_version: str, to_version: str) -> List[Dict[str, str]]:
        """è·å–ç‰ˆæœ¬è¿ç§»è·¯å¾„"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾ç‰ˆæœ¬å·æ˜¯é€’å¢çš„
        from_parts = [int(x) for x in from_version.split('.')]
        to_parts = [int(x) for x in to_version.split('.')]

        if from_parts >= to_parts:
            return []

        path = []
        current = from_parts[:]

        while current < to_parts:
            next_version = current[:]
            if current[2] < 9:  # è¡¥ä¸ç‰ˆæœ¬
                next_version[2] += 1
            elif current[1] < 9:  # æ¬¡ç‰ˆæœ¬
                next_version[1] += 1
                next_version[2] = 0
            else:  # ä¸»ç‰ˆæœ¬
                next_version[0] += 1
                next_version[1] = 0
                next_version[2] = 0

            if next_version <= to_parts:
                path.append({
                    'from': '.'.join(map(str, current)),
                    'to': '.'.join(map(str, next_version))
                })
                current = next_version
            else:
                break

        return path

    @staticmethod
    def _apply_migration_rule(workflow_data: Dict, rule: Dict) -> Tuple[Dict, List[str]]:
        """åº”ç”¨è¿ç§»è§„åˆ™"""
        migrated_data = workflow_data.copy()
        migration_log = []

        # æ·»åŠ å­—æ®µ
        if 'add_fields' in rule:
            for field_path, default_value in rule['add_fields'].items():
                if WorkflowVersionManager._set_nested_field(migrated_data, field_path, default_value):
                    migration_log.append(f"æ·»åŠ å­—æ®µ: {field_path}")

        return migrated_data, migration_log

    @staticmethod
    def _set_nested_field(data: Dict, field_path: str, value) -> bool:
        """è®¾ç½®åµŒå¥—å­—æ®µå€¼"""
        try:
            keys = field_path.split('.')
            current = data

            # éå†åˆ°å€’æ•°ç¬¬äºŒä¸ªé”®
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]

            # è®¾ç½®æœ€åä¸€ä¸ªé”®çš„å€¼
            final_key = keys[-1]
            if final_key not in current:
                current[final_key] = value
                return True

            return False  # å­—æ®µå·²å­˜åœ¨
        except Exception:
            return False

    @staticmethod
    def generate_version_report(workflow_data: Dict, output_path: str = None) -> str:
        """ç”Ÿæˆç‰ˆæœ¬åˆ†ææŠ¥å‘Š"""
        current_version = WorkflowVersionManager.parse_dsl_version(workflow_data)
        filename_version = WorkflowVersionManager.extract_version_from_filename(
            workflow_data.get('app', {}).get('name', 'unknown')
        )

        report_lines = [
            "# å·¥ä½œæµç‰ˆæœ¬åˆ†ææŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
            "",
            "## ç‰ˆæœ¬ä¿¡æ¯",
            f"- DSLç‰ˆæœ¬: {current_version}",
            f"- æ–‡ä»¶åç‰ˆæœ¬: {filename_version}",
            f"- ç‰ˆæœ¬ä¸€è‡´æ€§: {'[OK]' if current_version.replace('.', '') in filename_version else '[MISMATCH]'}",
            "",
            "## å…¼å®¹æ€§æ£€æŸ¥",
        ]

        if current_version in WorkflowVersionManager.VERSION_COMPATIBILITY:
            version_info = WorkflowVersionManager.VERSION_COMPATIBILITY[current_version]
            compatible_versions = version_info["compatible_versions"]
            features = version_info["features"]

            report_lines.extend([
                f"- å…¼å®¹ç‰ˆæœ¬: {', '.join(compatible_versions)}",
                f"- æ”¯æŒåŠŸèƒ½: {', '.join(features)}",
                "",
                "## æ¨èæ“ä½œ",
            ])

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§
            latest_version = max(WorkflowVersionManager.VERSION_COMPATIBILITY.keys())
            if current_version != latest_version:
                report_lines.append(f"- å»ºè®®å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬: {latest_version}")
            else:
                report_lines.append("- å½“å‰å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")

        report_content = "\n".join(report_lines)

        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
            except Exception as e:
                print(f"ä¿å­˜ç‰ˆæœ¬æŠ¥å‘Šå¤±è´¥: {e}")

        return report_content
    
    @staticmethod
    def generate_output_dirname(yaml_file: str, base_name: str = "parsed_workflow") -> str:
        """ç”Ÿæˆå¸¦ç‰ˆæœ¬å·å’Œæ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•å"""
        version = WorkflowVersionManager.extract_version_from_filename(yaml_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{base_name}_{version}_{timestamp}"
    
    @staticmethod
    def parse_dirname_info(dirname: str) -> Dict[str, str]:
        """è§£æç›®å½•åä¸­çš„ç‰ˆæœ¬å’Œæ—¶é—´ä¿¡æ¯"""
        # åŒ¹é…æ ¼å¼: parsed_workflow_V4.1_20240115_143022
        pattern = r'(.+)_(V\d+\.\d+)_(\d{8}_\d{6})'
        match = re.match(pattern, dirname)
        
        if match:
            return {
                'base_name': match.group(1),
                'version': match.group(2),
                'timestamp': match.group(3),
                'datetime': datetime.strptime(match.group(3), "%Y%m%d_%H%M%S")
            }
        return {}


class ToolManager:
    """Difyå·¥å…·ç®¡ç†å™¨ - æ”¯æŒå†…ç½®å·¥å…·å’ŒMCPå·¥å…·"""

    # å†…ç½®å·¥å…·å®šä¹‰
    BUILT_IN_TOOLS = {
        'web_search': {
            'name': 'web_search',
            'description': 'ç½‘é¡µæœç´¢å·¥å…·',
            'parameters': {
                'query': {'type': 'string', 'required': True, 'description': 'æœç´¢æŸ¥è¯¢'},
                'max_results': {'type': 'integer', 'required': False, 'default': 5},
                'engine': {'type': 'string', 'required': False, 'default': 'google'}
            },
            'credentials': ['api_key']
        },
        'calculator': {
            'name': 'calculator',
            'description': 'è®¡ç®—å™¨å·¥å…·',
            'parameters': {
                'expression': {'type': 'string', 'required': True, 'description': 'æ•°å­¦è¡¨è¾¾å¼'}
            }
        },
        'database_query': {
            'name': 'database_query',
            'description': 'æ•°æ®åº“æŸ¥è¯¢å·¥å…·',
            'parameters': {
                'query': {'type': 'string', 'required': True, 'description': 'SQLæŸ¥è¯¢è¯­å¥'},
                'connection_string': {'type': 'string', 'required': True, 'description': 'æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²'}
            },
            'credentials': ['db_username', 'db_password']
        },
        'file_operation': {
            'name': 'file_operation',
            'description': 'æ–‡ä»¶æ“ä½œå·¥å…·',
            'parameters': {
                'operation': {'type': 'string', 'required': True, 'enum': ['read', 'write', 'delete', 'list']},
                'file_path': {'type': 'string', 'required': True, 'description': 'æ–‡ä»¶è·¯å¾„'},
                'content': {'type': 'string', 'required': False, 'description': 'å†™å…¥å†…å®¹'}
            }
        },
        'email_sender': {
            'name': 'email_sender',
            'description': 'é‚®ä»¶å‘é€å·¥å…·',
            'parameters': {
                'to': {'type': 'string', 'required': True, 'description': 'æ”¶ä»¶äººé‚®ç®±'},
                'subject': {'type': 'string', 'required': True, 'description': 'é‚®ä»¶ä¸»é¢˜'},
                'body': {'type': 'string', 'required': True, 'description': 'é‚®ä»¶å†…å®¹'}
            },
            'credentials': ['smtp_server', 'smtp_username', 'smtp_password']
        },
        'api_caller': {
            'name': 'api_caller',
            'description': 'APIè°ƒç”¨å·¥å…·',
            'parameters': {
                'url': {'type': 'string', 'required': True, 'description': 'API URL'},
                'method': {'type': 'string', 'required': False, 'default': 'GET', 'enum': ['GET', 'POST', 'PUT', 'DELETE']},
                'headers': {'type': 'object', 'required': False, 'description': 'è¯·æ±‚å¤´'},
                'body': {'type': 'object', 'required': False, 'description': 'è¯·æ±‚ä½“'}
            },
            'credentials': ['api_key', 'bearer_token']
        }
    }

    # MCPå·¥å…·ç±»å‹æ˜ å°„
    MCP_TOOL_TYPES = {
        'mcp-server': 'Model Context Protocol Server',
        'mcp-client': 'Model Context Protocol Client',
        'mcp-resource': 'MCP Resource Access',
        'mcp-tool': 'MCP Tool Execution'
    }

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.discovered_tools = {}  # å·²å‘ç°çš„å·¥å…·
        self.tool_dependencies = {}  # å·¥å…·ä¾èµ–å…³ç³»

    def analyze_tools_in_workflow(self, workflow_data: Dict) -> Dict[str, Any]:
        """åˆ†æå·¥ä½œæµä¸­çš„å·¥å…·ä½¿ç”¨æƒ…å†µ"""
        tools_analysis = {
            'built_in_tools': [],
            'mcp_tools': [],
            'custom_tools': [],
            'tool_dependencies': {},
            'credentials_required': [],
            'statistics': {
                'total_tools': 0,
                'enabled_tools': 0,
                'tools_with_credentials': 0
            }
        }

        graph = workflow_data.get('workflow', {}).get('graph', {})
        nodes = graph.get('nodes', [])

        for node in nodes:
            node_data = node.get('data', {})
            node_type = node_data.get('type')

            if node_type == 'tool':
                tools_analysis['statistics']['total_tools'] += 1
                tool_config = self._analyze_tool_node(node_data, node.get('id'))
                tools_analysis['built_in_tools'].append(tool_config)

            elif node_type == 'mcp-server':
                tools_analysis['statistics']['total_tools'] += 1
                mcp_config = self._analyze_mcp_node(node_data, node.get('id'))
                tools_analysis['mcp_tools'].append(mcp_config)

            elif node_type == 'agent':
                # AgentèŠ‚ç‚¹å¯èƒ½åŒ…å«å·¥å…·é…ç½®
                agent_tools = self._analyze_agent_tools(node_data, node.get('id'))
                tools_analysis['custom_tools'].extend(agent_tools)
                tools_analysis['statistics']['total_tools'] += len(agent_tools)

        # ç»Ÿè®¡ä¿¡æ¯
        tools_analysis['statistics'] = self._calculate_tool_statistics(tools_analysis)

        # åˆ†æå·¥å…·ä¾èµ–å…³ç³»
        tools_analysis['tool_dependencies'] = self._analyze_tool_dependencies(workflow_data)

        return tools_analysis

    def _analyze_tool_node(self, node_data: Dict, node_id: str) -> Dict[str, Any]:
        """åˆ†æå·¥å…·èŠ‚ç‚¹"""
        tool_config = {
            'node_id': node_id,
            'tool_name': node_data.get('tool_name', 'unknown'),
            'provider': node_data.get('provider', 'unknown'),
            'enabled': node_data.get('enabled', True),
            'parameters': node_data.get('tool_parameters', {}),
            'credentials': node_data.get('tool_credential', {}),
            'has_credentials': bool(node_data.get('tool_credential')),
            'is_valid': self._validate_tool_config(node_data)
        }

        # æ£€æŸ¥æ˜¯å¦ä¸ºå†…ç½®å·¥å…·
        if tool_config['tool_name'] in self.BUILT_IN_TOOLS:
            built_in_def = self.BUILT_IN_TOOLS[tool_config['tool_name']]
            tool_config['is_built_in'] = True
            tool_config['description'] = built_in_def['description']
            tool_config['required_credentials'] = built_in_def.get('credentials', [])
        else:
            tool_config['is_built_in'] = False
            tool_config['description'] = 'è‡ªå®šä¹‰å·¥å…·'

        return tool_config

    def _analyze_mcp_node(self, node_data: Dict, node_id: str) -> Dict[str, Any]:
        """åˆ†æMCPèŠ‚ç‚¹"""
        mcp_config = {
            'node_id': node_id,
            'mcp_server_ids': node_data.get('mcp_server_ids', []),
            'mcp_tools': node_data.get('mcp_tools', []),
            'enabled': True,
            'type': 'mcp-server',
            'description': self.MCP_TOOL_TYPES.get('mcp-server', 'MCP Server'),
            'is_valid': self._validate_mcp_config(node_data)
        }

        return mcp_config

    def _analyze_agent_tools(self, node_data: Dict, node_id: str) -> List[Dict[str, Any]]:
        """åˆ†æAgentèŠ‚ç‚¹ä¸­çš„å·¥å…·"""
        tools = []
        agent_parameters = node_data.get('agent_parameters', {})

        # æ£€æŸ¥å·¥å…·å‚æ•°
        if isinstance(agent_parameters, dict):
            tools_param = agent_parameters.get('tools', [])
            if isinstance(tools_param, list):
                for i, tool in enumerate(tools_param):
                    tool_config = {
                        'node_id': f"{node_id}_tool_{i}",
                        'tool_name': tool.get('name', f'agent_tool_{i}'),
                        'provider': 'agent',
                        'enabled': tool.get('enabled', True),
                        'parameters': tool.get('parameters', {}),
                        'settings': tool.get('settings', {}),
                        'is_built_in': False,
                        'description': f"Agentå·¥å…·: {tool.get('name', 'unknown')}",
                        'has_credentials': bool(tool.get('settings')),
                        'is_valid': True
                    }
                    tools.append(tool_config)

        return tools

    def _validate_tool_config(self, node_data: Dict) -> bool:
        """éªŒè¯å·¥å…·é…ç½®"""
        try:
            tool_name = node_data.get('tool_name')
            if not tool_name:
                return False

            # å¦‚æœæ˜¯å†…ç½®å·¥å…·ï¼Œè¿›è¡Œæ›´ä¸¥æ ¼çš„éªŒè¯
            if tool_name in self.BUILT_IN_TOOLS:
                built_in_def = self.BUILT_IN_TOOLS[tool_name]
                required_params = [k for k, v in built_in_def['parameters'].items() if v.get('required', False)]

                tool_params = node_data.get('tool_parameters', {})
                for param in required_params:
                    if param not in tool_params:
                        self.logger.warning(f"å·¥å…· {tool_name} ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")
                        return False

            return True
        except Exception as e:
            self.logger.error(f"å·¥å…·é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False

    def _validate_mcp_config(self, node_data: Dict) -> bool:
        """éªŒè¯MCPé…ç½®"""
        try:
            mcp_server_ids = node_data.get('mcp_server_ids', [])
            if not mcp_server_ids:
                self.logger.warning("MCPèŠ‚ç‚¹ç¼ºå°‘æœåŠ¡å™¨IDé…ç½®")
                return False

            # æ£€æŸ¥MCPæœåŠ¡å™¨IDæ ¼å¼
            for server_id in mcp_server_ids:
                if not isinstance(server_id, str) or not server_id.strip():
                    return False

            return True
        except Exception as e:
            self.logger.error(f"MCPé…ç½®éªŒè¯å¤±è´¥: {e}")
            return False

    def _calculate_tool_statistics(self, tools_analysis: Dict) -> Dict[str, int]:
        """è®¡ç®—å·¥å…·ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—æ€»å·¥å…·æ•°é‡
        total_tools = (len(tools_analysis.get('built_in_tools', [])) +
                      len(tools_analysis.get('mcp_tools', [])) +
                      len(tools_analysis.get('custom_tools', [])))

        stats = {
            'total_tools': total_tools,
            'enabled_tools': 0,
            'tools_with_credentials': 0,
            'built_in_tools_count': len(tools_analysis.get('built_in_tools', [])),
            'mcp_tools_count': len(tools_analysis.get('mcp_tools', [])),
            'custom_tools_count': len(tools_analysis.get('custom_tools', []))
        }

        # è®¡ç®—å¯ç”¨å·¥å…·æ•°é‡
        for tool in tools_analysis.get('built_in_tools', []):
            if tool.get('enabled', True):
                stats['enabled_tools'] += 1
            if tool.get('has_credentials'):
                stats['tools_with_credentials'] += 1

        for tool in tools_analysis.get('custom_tools', []):
            if tool.get('enabled', True):
                stats['enabled_tools'] += 1
            if tool.get('has_credentials'):
                stats['tools_with_credentials'] += 1

        # MCPå·¥å…·é»˜è®¤å¯ç”¨
        stats['enabled_tools'] += len(tools_analysis.get('mcp_tools', []))

        return stats

    def _analyze_tool_dependencies(self, workflow_data: Dict) -> Dict[str, List[str]]:
        """åˆ†æå·¥å…·ä¾èµ–å…³ç³»"""
        dependencies = {}

        graph = workflow_data.get('workflow', {}).get('graph', {})
        nodes = graph.get('nodes', [])
        edges = graph.get('edges', [])

        # æ„å»ºèŠ‚ç‚¹ä¾èµ–å›¾
        for edge in edges:
            source_id = edge.get('source')
            target_id = edge.get('target')

            if source_id not in dependencies:
                dependencies[source_id] = []
            dependencies[source_id].append(target_id)

        # è¯†åˆ«å·¥å…·ç›¸å…³çš„ä¾èµ–
        tool_dependencies = {}
        for node in nodes:
            node_id = node.get('id')
            node_type = node.get('data', {}).get('type')

            if node_type in ['tool', 'mcp-server', 'agent']:
                tool_dependencies[node_id] = dependencies.get(node_id, [])

        return tool_dependencies

    def generate_tool_report(self, tools_analysis: Dict, output_path: str = None) -> str:
        """ç”Ÿæˆå·¥å…·ä½¿ç”¨æŠ¥å‘Š"""
        report_lines = [
            "# Difyå·¥ä½œæµå·¥å…·åˆ†ææŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}",
            "",
            "## å·¥å…·ç»Ÿè®¡",
            f"- æ€»å·¥å…·æ•°é‡: {tools_analysis['statistics']['total_tools']}",
            f"- å¯ç”¨å·¥å…·æ•°é‡: {tools_analysis['statistics']['enabled_tools']}",
            f"- å†…ç½®å·¥å…·æ•°é‡: {tools_analysis['statistics']['built_in_tools_count']}",
            f"- MCPå·¥å…·æ•°é‡: {tools_analysis['statistics']['mcp_tools_count']}",
            f"- è‡ªå®šä¹‰å·¥å…·æ•°é‡: {tools_analysis['statistics']['custom_tools_count']}",
            f"- éœ€è¦å‡­æ®çš„å·¥å…·: {tools_analysis['statistics']['tools_with_credentials']}",
            "",
            "## å†…ç½®å·¥å…·è¯¦æƒ…",
            ""
        ]

        for tool in tools_analysis.get('built_in_tools', []):
            status = "[ENABLED]" if tool.get('enabled') else "[DISABLED]"
            cred_status = "ğŸ” æœ‰å‡­æ®" if tool.get('has_credentials') else "ğŸ”“ æ— å‡­æ®"
            valid_status = "[VALID]" if tool.get('is_valid') else "[INVALID]"

            report_lines.extend([
                f"### {tool['tool_name']} ({tool['node_id']})",
                f"- çŠ¶æ€: {status}",
                f"- å‡­æ®: {cred_status}",
                f"- é…ç½®: {valid_status}",
                f"- æä¾›å•†: {tool['provider']}",
                f"- æè¿°: {tool['description']}",
                ""
            ])

        if tools_analysis.get('mcp_tools'):
            report_lines.extend([
                "## MCPå·¥å…·è¯¦æƒ…",
                ""
            ])

            for tool in tools_analysis['mcp_tools']:
                valid_status = "[VALID]" if tool.get('is_valid') else "[INVALID]"
                report_lines.extend([
                    f"### MCP Server ({tool['node_id']})",
                    f"- é…ç½®: {valid_status}",
                    f"- æœåŠ¡å™¨IDs: {', '.join(tool['mcp_server_ids'])}",
                    f"- å·¥å…·æ•°é‡: {len(tool.get('mcp_tools', []))}",
                    ""
                ])

        if tools_analysis.get('custom_tools'):
            report_lines.extend([
                "## è‡ªå®šä¹‰å·¥å…·è¯¦æƒ…",
                ""
            ])

            for tool in tools_analysis['custom_tools']:
                status = "[ENABLED]" if tool.get('enabled') else "[DISABLED]"
                cred_status = "ğŸ” æœ‰å‡­æ®" if tool.get('has_credentials') else "ğŸ”“ æ— å‡­æ®"

                report_lines.extend([
                    f"### {tool['tool_name']} ({tool['node_id']})",
                    f"- çŠ¶æ€: {status}",
                    f"- å‡­æ®: {cred_status}",
                    f"- æä¾›å•†: {tool['provider']}",
                    f"- æè¿°: {tool['description']}",
                    ""
                ])

        report_content = "\n".join(report_lines)

        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                self.logger.info(f"å·¥å…·æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜å·¥å…·æŠ¥å‘Šå¤±è´¥: {e}")

        return report_content


class WorkflowParallelProcessor:
    """å·¥ä½œæµå¹¶è¡Œå¤„ç†å™¨"""

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(8, os.cpu_count() or 4)
        self.logger = logging.getLogger(__name__)

    def analyze_dependencies(self, workflow_data: Dict) -> Dict[str, List[str]]:
        """åˆ†æèŠ‚ç‚¹ä¾èµ–å…³ç³»

        Returns:
            èŠ‚ç‚¹ä¾èµ–å›¾ï¼š{node_id: [dependent_node_ids]}
        """
        graph = workflow_data.get('workflow', {}).get('graph', {})
        edges = graph.get('edges', [])

        # æ„å»ºä¾èµ–å›¾ï¼šnode_id -> [ä¾èµ–æ­¤èŠ‚ç‚¹çš„ä¸‹æ¸¸èŠ‚ç‚¹]
        dependencies = {}

        for edge in edges:
            source_id = edge.get('source')
            target_id = edge.get('target')

            if source_id not in dependencies:
                dependencies[source_id] = []
            if target_id not in dependencies:
                dependencies[target_id] = []

            # targetä¾èµ–sourceï¼Œæ‰€ä»¥source -> target
            dependencies[source_id].append(target_id)

        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨ä¾èµ–å›¾ä¸­
        nodes = graph.get('nodes', [])
        for node in nodes:
            node_id = node.get('id')
            if node_id not in dependencies:
                dependencies[node_id] = []

        return dependencies

    def get_execution_levels(self, dependencies: Dict[str, List[str]]) -> List[List[str]]:
        """è·å–æ‰§è¡Œå±‚çº§ï¼ˆæ‹“æ‰‘æ’åºï¼‰

        Returns:
            æ‰§è¡Œå±‚çº§åˆ—è¡¨ï¼Œæ¯ä¸ªå±‚çº§åŒ…å«å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹
        """
        # è®¡ç®—å…¥åº¦
        indegree = {node: 0 for node in dependencies}
        for node in dependencies:
            for dependent in dependencies[node]:
                indegree[dependent] += 1

        # æ‹“æ‰‘æ’åº
        levels = []
        current_level = [node for node in indegree if indegree[node] == 0]

        while current_level:
            levels.append(current_level[:])  # å¤åˆ¶å½“å‰å±‚çº§

            next_level = []
            for node in current_level:
                for dependent in dependencies[node]:
                    indegree[dependent] -= 1
                    if indegree[dependent] == 0:
                        next_level.append(dependent)

            current_level = next_level

        return levels

    def process_nodes_parallel(self, nodes: List[Dict], process_func, **kwargs) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†èŠ‚ç‚¹

        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶(node, **kwargs)
            **kwargs: ä¼ é€’ç»™å¤„ç†å‡½æ•°çš„å‚æ•°

        Returns:
            {node_id: result}
        """
        results = {}
        lock = threading.Lock()

        def process_single_node(node):
            try:
                result = process_func(node, **kwargs)
                with lock:
                    results[node['id']] = result
                return node['id'], result
            except Exception as e:
                self.logger.error(f"å¤„ç†èŠ‚ç‚¹ {node['id']} å¤±è´¥: {e}")
                with lock:
                    results[node['id']] = None
                return node['id'], None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_single_node, node) for node in nodes]

            for future in as_completed(futures):
                try:
                    node_id, result = future.result()
                    if result is None:
                        self.logger.warning(f"èŠ‚ç‚¹ {node_id} å¤„ç†å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"èŠ‚ç‚¹å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")

        return results


class WorkflowSplitter:
    """å·¥ä½œæµæ‹†åˆ†å™¨ - å¢å¼ºç‰ˆï¼Œæ”¯æŒåŸå§‹æ ¼å¼ä¿ç•™å’Œä½ç½®æ˜ å°„"""
    
    def __init__(self, yaml_file: str, output_dir: str = None, enable_parallel: bool = True):
        self.yaml_file = Path(yaml_file)
        if output_dir is None:
            output_dir = WorkflowVersionManager.generate_output_dirname(str(yaml_file))
        self.output_dir = Path(output_dir)

        self.workflow_data = None
        self.raw_yaml_lines = []  # åŸå§‹YAMLè¡Œ
        self.node_positions = {}  # èŠ‚ç‚¹ä½ç½®æ˜ å°„
        self.metadata = {}

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.enable_parallel = enable_parallel
        self.parallel_processor = WorkflowParallelProcessor() if enable_parallel else None

        # é”™è¯¯å¤„ç†
        self.error_handler = ErrorHandler()

        # å·¥å…·ç®¡ç†
        self.tool_manager = ToolManager()

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # åˆå§‹åŒ–YAMLå¤„ç†å™¨
        if YAML_AVAILABLE:
            self.yaml_processor = YAML()
            self.yaml_processor.preserve_quotes = True
            self.yaml_processor.width = 4096
        else:
            self.yaml_processor = None
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.output_dir / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(log_dir / 'workflow_manager.log', encoding='utf-8')
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def split_workflow(self, validate_dsl: bool = True) -> bool:
        """æ‹†åˆ†å·¥ä½œæµ

        Args:
            validate_dsl: æ˜¯å¦éªŒè¯DSLç»“æ„
        """
        def _split_operation():
            self.logger.info(f"å¼€å§‹æ‹†åˆ†å·¥ä½œæµ: {self.yaml_file}")

            # è¯»å–å’Œè§£æYAML
            if not self._load_yaml():
                raise Exception("YAMLæ–‡ä»¶åŠ è½½å¤±è´¥")

            # DSLç»“æ„éªŒè¯
            if validate_dsl:
                self.logger.info("å¼€å§‹éªŒè¯DSLç»“æ„...")
                is_valid, validation_errors = DSLValidator.validate_dsl_structure(self.workflow_data)
                if not is_valid:
                    error_msg = f"DSLç»“æ„éªŒè¯å¤±è´¥: {', '.join(validation_errors)}"
                    self.logger.error(error_msg)
                    raise Exception(error_msg)
                self.logger.info("DSLç»“æ„éªŒè¯é€šè¿‡")

            # åˆ›å»ºè¾“å‡ºç»“æ„
            self._create_output_structure()

            # ç”Ÿæˆå…ƒæ•°æ®
            self._generate_metadata()

            # æ‹†åˆ†èŠ‚ç‚¹
            success = self._split_nodes()
            if not success:
                raise Exception("èŠ‚ç‚¹æ‹†åˆ†å¤±è´¥")

            # ä¿å­˜å…ƒæ•°æ®å’Œæ˜ å°„ä¿¡æ¯
            self._save_metadata()

            self.logger.info(f"å·¥ä½œæµæ‹†åˆ†å®Œæˆ: {self.output_dir}")
            return True

        try:
            return self.error_handler.handle_operation(_split_operation, "å·¥ä½œæµæ‹†åˆ†")
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµæ‹†åˆ†å¤±è´¥: {e}")
            # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            error_report = self.error_handler.generate_error_report(
                self.output_dir / "logs" / "error_report.md" if self.output_dir.exists() else None
            )
            if error_report != "æ— é”™è¯¯è®°å½•":
                self.logger.info("é”™è¯¯æŠ¥å‘Šå·²ç”Ÿæˆ")
            return False
    
    def _load_yaml(self) -> bool:
        """åŠ è½½å’Œè§£æYAMLæ–‡ä»¶"""
        try:
            # è¯»å–åŸå§‹æ–‡æœ¬è¡Œï¼ˆç”¨äºä½ç½®æ˜ å°„ï¼‰
            with open(self.yaml_file, 'r', encoding='utf-8') as f:
                self.raw_yaml_lines = f.readlines()
            
            # è§£æYAMLç»“æ„
            if YAML_AVAILABLE:
                with open(self.yaml_file, 'r', encoding='utf-8') as f:
                    self.workflow_data = self.yaml_processor.load(f)
            else:
                with open(self.yaml_file, 'r', encoding='utf-8') as f:
                    self.workflow_data = yaml.safe_load(f)
            
            # æå–åŸºæœ¬ä¿¡æ¯
            workflow_graph = self.workflow_data.get('workflow', {}).get('graph', {})
            nodes = workflow_graph.get('nodes', [])
            edges = workflow_graph.get('edges', [])
            
            self.logger.info(f"YAMLè§£ææˆåŠŸ: {len(nodes)} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edges)} ä¸ªè¿æ¥")
            return True
            
        except Exception as e:
            self.logger.error(f"YAMLè§£æå¤±è´¥: {e}")
            return False
    
    def _create_output_structure(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        # ä¸»ç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­ç›®å½•
        (self.output_dir / "metadata").mkdir(exist_ok=True)
        (self.output_dir / "nodes").mkdir(exist_ok=True)
        (self.output_dir / "tools").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        # èŠ‚ç‚¹ç±»å‹ç›®å½• - åŸºäºDifyæ”¯æŒçš„40+ç§èŠ‚ç‚¹ç±»å‹
        node_types = [
            # åŸºç¡€èŠ‚ç‚¹
            'start', 'end', 'answer',

            # æ•°æ®å¤„ç†èŠ‚ç‚¹
            'llm', 'code', 'template_transform', 'variable_aggregator',
            'variable_assigner', 'knowledge_retrieval',

            # é€»è¾‘æ§åˆ¶èŠ‚ç‚¹
            'if_else', 'iteration', 'iteration_start', 'loop',

            # å¤–éƒ¨é›†æˆèŠ‚ç‚¹
            'http_request', 'tool', 'agent', 'question_classifier',
            'mcp_server', 'webhook', 'scheduled',

            # è§¦å‘å™¨èŠ‚ç‚¹
            'trigger_webhook', 'trigger_schedule', 'trigger_plugin',

            # å…¶ä»–èŠ‚ç‚¹
            'parameter_extractor', 'list_filter', 'doc_extractor',
            'unknown'
        ]
        
        for node_type in node_types:
            (self.output_dir / "nodes" / node_type).mkdir(exist_ok=True)
    
    def _generate_metadata(self):
        """ç”Ÿæˆå…ƒæ•°æ®ä¿¡æ¯"""
        app_info = self.workflow_data.get('app', {})
        workflow_info = self.workflow_data.get('workflow', {})

        # åˆ†æå·¥å…·ä½¿ç”¨æƒ…å†µ
        tools_analysis = self.tool_manager.analyze_tools_in_workflow(self.workflow_data)

        self.metadata = {
            'source_file': str(self.yaml_file),
            'version_info': {
                'extracted_version': WorkflowVersionManager.extract_version_from_filename(self.yaml_file.name),
                'dsl_version': WorkflowVersionManager.parse_dsl_version(self.workflow_data),
                'app_name': app_info.get('name', 'unknown'),
                'app_mode': app_info.get('mode', 'unknown'),
                'compatibility_check': WorkflowVersionManager.check_version_compatibility(
                    WorkflowVersionManager.parse_dsl_version(self.workflow_data),
                    WorkflowVersionManager.parse_dsl_version(self.workflow_data)
                )
            },
            'split_info': {
                'timestamp': datetime.now().isoformat(),
                'output_directory': str(self.output_dir),
                'total_lines': len(self.raw_yaml_lines)
            },
            'statistics': {
                'total_nodes': 0,
                'total_edges': 0,
                'node_types': {}
            },
            'tools_analysis': tools_analysis
        }
    
    def _split_nodes(self) -> bool:
        """æ‹†åˆ†èŠ‚ç‚¹ - æ”¯æŒå¹¶è¡Œå¤„ç†"""
        try:
            workflow_graph = self.workflow_data.get('workflow', {}).get('graph', {})
            nodes = workflow_graph.get('nodes', [])

            node_stats = {}

            if self.enable_parallel and len(nodes) > 3:  # åªæœ‰åœ¨èŠ‚ç‚¹è¾ƒå¤šæ—¶æ‰ä½¿ç”¨å¹¶è¡Œå¤„ç†
                self.logger.info("ä½¿ç”¨å¹¶è¡Œå¤„ç†æ‹†åˆ†èŠ‚ç‚¹...")

                # å¹¶è¡Œå¤„ç†èŠ‚ç‚¹
                def process_node_wrapper(node_with_index):
                    node, i = node_with_index
                    node_data = node.get('data', {})
                    node_type = node_data.get('type', 'unknown')
                    node_id = node.get('id', f'unknown_{i}')
                    node_title = node_data.get('title', f'èŠ‚ç‚¹_{i}')

                    # æ‹†åˆ†èŠ‚ç‚¹
                    success = self._split_single_node(node, node_type, i + 1)  # ä½¿ç”¨å®é™…ç´¢å¼•+1
                    return {
                        'node_id': node_id,
                        'node_type': node_type,
                        'success': success,
                        'index': i
                    }

                # å‡†å¤‡èŠ‚ç‚¹æ•°æ®
                nodes_with_indices = [(node, i) for i, node in enumerate(nodes)]

                # å¹¶è¡Œå¤„ç†
                results = self.parallel_processor.process_nodes_parallel(
                    nodes_with_indices,
                    lambda node_with_index, **kwargs: process_node_wrapper(node_with_index)
                )

                # ç»Ÿè®¡ç»“æœ
                for node_id, result in results.items():
                    if result and result['success']:
                        node_stats[result['node_type']] = node_stats.get(result['node_type'], 0) + 1
                    else:
                        self.logger.warning(f"èŠ‚ç‚¹æ‹†åˆ†å¤±è´¥: {node_id}")

            else:
                # ä¸²è¡Œå¤„ç†
                self.logger.info("ä½¿ç”¨ä¸²è¡Œå¤„ç†æ‹†åˆ†èŠ‚ç‚¹...")
                for i, node in enumerate(nodes):
                    node_data = node.get('data', {})
                    node_type = node_data.get('type', 'unknown')
                    node_id = node.get('id', f'unknown_{i}')
                    node_title = node_data.get('title', f'èŠ‚ç‚¹_{i}')

                    # ç»Ÿè®¡
                    node_stats[node_type] = node_stats.get(node_type, 0) + 1
                    current_index = node_stats[node_type]

                    # æ‹†åˆ†èŠ‚ç‚¹
                    success = self._split_single_node(node, node_type, current_index)
                    if not success:
                        self.logger.warning(f"èŠ‚ç‚¹æ‹†åˆ†å¤±è´¥: {node_id}")

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.metadata['statistics']['total_nodes'] = len(nodes)
            self.metadata['statistics']['total_edges'] = len(workflow_graph.get('edges', []))
            self.metadata['statistics']['node_types'] = node_stats
            self.metadata['statistics']['parallel_processing'] = self.enable_parallel

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†èŠ‚ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _split_single_node(self, node: Dict, node_type: str, index: int) -> bool:
        """æ‹†åˆ†å•ä¸ªèŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            
            # åˆ›å»ºèŠ‚ç‚¹ç›®å½•
            node_dir_name = f"{index:02d}_{node_type}_{node_id}"
            node_dir = self.output_dir / "nodes" / node_type / node_dir_name
            node_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆèŠ‚ç‚¹æ–‡ä»¶ - æ”¯æŒæ›´å¤šDifyèŠ‚ç‚¹ç±»å‹
            if node_type == 'llm':
                return self._split_llm_node(node, node_dir)
            elif node_type == 'code':
                return self._split_code_node(node, node_dir)
            elif node_type in ['mcp-server', 'mcp_server']:
                return self._split_mcp_node(node, node_dir)
            elif node_type == 'agent':
                return self._split_agent_node(node, node_dir)
            elif node_type == 'tool':
                return self._split_tool_node(node, node_dir)
            elif node_type == 'http_request':
                return self._split_http_request_node(node, node_dir)
            elif node_type == 'if_else':
                return self._split_if_else_node(node, node_dir)
            elif node_type == 'iteration':
                return self._split_iteration_node(node, node_dir)
            elif node_type == 'template_transform':
                return self._split_template_transform_node(node, node_dir)
            elif node_type in ['start', 'end']:
                return self._split_flow_control_node(node, node_dir)
            elif node_type.startswith('trigger_'):
                return self._split_trigger_node(node, node_dir)
            else:
                return self._split_generic_node(node, node_dir)
                
        except Exception as e:
            self.logger.error(f"æ‹†åˆ†å•ä¸ªèŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _split_llm_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†LLMèŠ‚ç‚¹ - ä¿ç•™åŸå§‹æ ¼å¼"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            
            # åŸºç¡€é…ç½®ï¼ˆå»é™¤æç¤ºè¯ï¼‰
            base_config = {k: v for k, v in node_data.items() 
                          if k not in ['prompt_template']}
            
            # ä¿å­˜åŸºç¡€é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)
            
            # ä¿å­˜æç¤ºè¯ï¼ˆåŸå§‹æ ¼å¼ï¼‰
            prompt_template = node_data.get('prompt_template', [])
            for i, prompt in enumerate(prompt_template):
                role = prompt.get('role', 'user')
                text = prompt.get('text', '')
                
                # ä¿å­˜ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä¿ç•™åŸå§‹æ ¼å¼
                prompt_file = node_dir / f'{role}_prompt_{i+1}.txt'
                with open(prompt_file, 'w', encoding='utf-8') as f:
                    f.write(text)
            
            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'llm',
                'title': node_data.get('title', ''),
                'position': node.get('position', {}),
                'prompt_count': len(prompt_template),
                'prompt_files': [f'{p.get("role", "user")}_prompt_{i+1}.txt' 
                               for i, p in enumerate(prompt_template)]
            }
            
            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'llm',
                'index_in_yaml': None  # å°†åœ¨åç»­ç‰ˆæœ¬ä¸­å®ç°è¡Œå·è¿½è¸ª
            }
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ‹†åˆ†LLMèŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _split_code_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†ä»£ç èŠ‚ç‚¹ - ä¿ç•™åŸå§‹æ ¼å¼"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            
            # åŸºç¡€é…ç½®ï¼ˆå»é™¤ä»£ç ï¼‰
            base_config = {k: v for k, v in node_data.items() 
                          if k not in ['code']}
            
            # ä¿å­˜åŸºç¡€é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)
            
            # ä¿å­˜ä»£ç ï¼ˆåŸå§‹æ ¼å¼ï¼‰
            code_content = node_data.get('code', '')
            code_language = node_data.get('code_language', 'python3')
            
            # æ ¹æ®è¯­è¨€ç¡®å®šæ–‡ä»¶æ‰©å±•å
            extension_map = {
                'python3': 'py',
                'python': 'py', 
                'javascript': 'js',
                'typescript': 'ts'
            }
            ext = extension_map.get(code_language, 'txt')
            
            with open(node_dir / f'code.{ext}', 'w', encoding='utf-8') as f:
                f.write(code_content)
            
            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'code',
                'title': node_data.get('title', ''),
                'code_language': code_language,
                'code_file': f'code.{ext}',
                'position': node.get('position', {})
            }
            
            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'code',
                'index_in_yaml': None
            }
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ‹†åˆ†ä»£ç èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _split_mcp_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†MCPèŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            
            # ä¿å­˜å®Œæ•´é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(node_data, f)
                else:
                    yaml.dump(node_data, f, default_flow_style=False, allow_unicode=True)
            
            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'mcp-server',
                'title': node_data.get('title', ''),
                'mcp_server_ids': node_data.get('mcp_server_ids', []),
                'position': node.get('position', {})
            }
            
            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'mcp-server',
                'index_in_yaml': None
            }
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ‹†åˆ†MCPèŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _split_agent_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†AgentèŠ‚ç‚¹ - åŸºäºDify AgentèŠ‚ç‚¹ç‰¹æ€§"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # åŸºç¡€é…ç½®ï¼ˆå»é™¤å¤æ‚å‚æ•°ï¼‰
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['agent_parameters', 'prompt_template']}

            # ä¿å­˜åŸºç¡€é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜Agentå‚æ•°
            agent_parameters = node_data.get('agent_parameters', {})
            if agent_parameters:
                with open(node_dir / 'agent_parameters.yaml', 'w', encoding='utf-8') as f:
                    if YAML_AVAILABLE:
                        self.yaml_processor.dump(agent_parameters, f)
                    else:
                        yaml.dump(agent_parameters, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜æç¤ºè¯æ¨¡æ¿
            prompt_template = node_data.get('prompt_template', [])
            for i, prompt in enumerate(prompt_template):
                role = prompt.get('role', 'user')
                text = prompt.get('text', '')

                prompt_file = node_dir / f'{role}_prompt_{i+1}.txt'
                with open(prompt_file, 'w', encoding='utf-8') as f:
                    f.write(text)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'agent',
                'title': node_data.get('title', ''),
                'agent_strategy_provider': node_data.get('agent_strategy_provider_name', ''),
                'agent_strategy': node_data.get('agent_strategy_name', ''),
                'position': node.get('position', {}),
                'prompt_count': len(prompt_template),
                'agent_parameters_file': 'agent_parameters.yaml' if agent_parameters else None,
                'prompt_files': [f'{p.get("role", "user")}_prompt_{i+1}.txt'
                               for i, p in enumerate(prompt_template)]
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'agent',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†AgentèŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_tool_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†å·¥å…·èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # å·¥å…·é…ç½®
            tool_config = {
                'provider': node_data.get('provider', ''),
                'tool_name': node_data.get('tool_name', ''),
                'tool_parameters': node_data.get('tool_parameters', {}),
                'tool_credential': node_data.get('tool_credential', {})
            }

            # ä¿å­˜å·¥å…·é…ç½®
            with open(node_dir / 'tool_config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(tool_config, f)
                else:
                    yaml.dump(tool_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®ï¼ˆå»é™¤å·¥å…·ç›¸å…³å‚æ•°ï¼‰
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['tool_parameters', 'tool_credential']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'tool',
                'title': node_data.get('title', ''),
                'provider': tool_config['provider'],
                'tool_name': tool_config['tool_name'],
                'position': node.get('position', {}),
                'config_files': ['tool_config.yaml', 'config.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'tool',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†å·¥å…·èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_http_request_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†HTTPè¯·æ±‚èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # HTTPè¯·æ±‚é…ç½®
            http_config = {
                'method': node_data.get('method', 'GET'),
                'url': node_data.get('url', ''),
                'headers': node_data.get('headers', []),
                'params': node_data.get('params', []),
                'body': node_data.get('body', {}),
                'authorization': node_data.get('authorization', {}),
                'timeout': node_data.get('timeout', 60)
            }

            # ä¿å­˜HTTPé…ç½®
            with open(node_dir / 'http_config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(http_config, f)
                else:
                    yaml.dump(http_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['headers', 'params', 'body', 'authorization']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'http_request',
                'title': node_data.get('title', ''),
                'method': http_config['method'],
                'url': http_config['url'],
                'position': node.get('position', {}),
                'config_files': ['http_config.yaml', 'config.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'http_request',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†HTTPè¯·æ±‚èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_if_else_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†æ¡ä»¶åˆ†æ”¯èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # æ¡ä»¶é€»è¾‘é…ç½®
            logic_config = {
                'conditions': node_data.get('conditions', []),
                'logical_operator': node_data.get('logical_operator', 'and'),
                'variable_selectors': node_data.get('variable_selectors', [])
            }

            # ä¿å­˜æ¡ä»¶é…ç½®
            with open(node_dir / 'logic_config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(logic_config, f)
                else:
                    yaml.dump(logic_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['conditions', 'variable_selectors']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'if_else',
                'title': node_data.get('title', ''),
                'conditions_count': len(logic_config['conditions']),
                'logical_operator': logic_config['logical_operator'],
                'position': node.get('position', {}),
                'config_files': ['logic_config.yaml', 'config.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'if_else',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†æ¡ä»¶åˆ†æ”¯èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_iteration_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†è¿­ä»£èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # è¿­ä»£é…ç½®
            iteration_config = {
                'iterator_selector': node_data.get('iterator_selector', []),
                'output_selector': node_data.get('output_selector', []),
                'start_node_id': node_data.get('start_node_id', ''),
                'output_type': node_data.get('output_type', 'array')
            }

            # ä¿å­˜è¿­ä»£é…ç½®
            with open(node_dir / 'iteration_config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(iteration_config, f)
                else:
                    yaml.dump(iteration_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['iterator_selector', 'output_selector']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'iteration',
                'title': node_data.get('title', ''),
                'start_node_id': iteration_config['start_node_id'],
                'output_type': iteration_config['output_type'],
                'position': node.get('position', {}),
                'config_files': ['iteration_config.yaml', 'config.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'iteration',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†è¿­ä»£èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_template_transform_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†æ¨¡æ¿è½¬æ¢èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')

            # æ¨¡æ¿é…ç½®
            template_config = {
                'template': node_data.get('template', ''),
                'variable_selectors': node_data.get('variable_selectors', [])
            }

            # ä¿å­˜æ¨¡æ¿å†…å®¹
            with open(node_dir / 'template.txt', 'w', encoding='utf-8') as f:
                f.write(template_config['template'])

            # ä¿å­˜å˜é‡é€‰æ‹©å™¨é…ç½®
            with open(node_dir / 'variable_selectors.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(template_config['variable_selectors'], f)
                else:
                    yaml.dump(template_config['variable_selectors'], f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['template', 'variable_selectors']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': 'template_transform',
                'title': node_data.get('title', ''),
                'template_file': 'template.txt',
                'variable_selectors_file': 'variable_selectors.yaml',
                'position': node.get('position', {}),
                'config_files': ['config.yaml', 'template.txt', 'variable_selectors.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': 'template_transform',
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†æ¨¡æ¿è½¬æ¢èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_flow_control_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†æµç¨‹æ§åˆ¶èŠ‚ç‚¹ï¼ˆstart/endï¼‰"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            node_type = node_data.get('type', 'unknown')

            # ä¿å­˜å®Œæ•´é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(node_data, f)
                else:
                    yaml.dump(node_data, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': node_type,
                'title': node_data.get('title', ''),
                'position': node.get('position', {}),
                'is_flow_control': True,
                'flow_role': node_type  # start æˆ– end
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': node_type,
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†æµç¨‹æ§åˆ¶èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_trigger_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†è§¦å‘å™¨èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            node_type = node_data.get('type', 'unknown')

            # è§¦å‘å™¨é…ç½®
            trigger_config = {
                'trigger_type': node_type.replace('trigger_', ''),
                'config': node_data.get('config', {}),
                'schedule_config': node_data.get('schedule_config', {}),
                'webhook_config': node_data.get('webhook_config', {})
            }

            # ä¿å­˜è§¦å‘å™¨é…ç½®
            with open(node_dir / 'trigger_config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(trigger_config, f)
                else:
                    yaml.dump(trigger_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜åŸºç¡€é…ç½®
            base_config = {k: v for k, v in node_data.items()
                          if k not in ['config', 'schedule_config', 'webhook_config']}
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(base_config, f)
                else:
                    yaml.dump(base_config, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': node_type,
                'title': node_data.get('title', ''),
                'trigger_type': trigger_config['trigger_type'],
                'position': node.get('position', {}),
                'config_files': ['trigger_config.yaml', 'config.yaml']
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': node_type,
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†è§¦å‘å™¨èŠ‚ç‚¹å¤±è´¥: {e}")
            return False

    def _split_generic_node(self, node: Dict, node_dir: Path) -> bool:
        """æ‹†åˆ†é€šç”¨èŠ‚ç‚¹"""
        try:
            node_data = node.get('data', {})
            node_id = node.get('id', 'unknown')
            node_type = node_data.get('type', 'unknown')

            # ä¿å­˜å®Œæ•´é…ç½®
            with open(node_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(node_data, f)
                else:
                    yaml.dump(node_data, f, default_flow_style=False, allow_unicode=True)

            # ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®
            metadata = {
                'node_id': node_id,
                'node_type': node_type,
                'title': node_data.get('title', ''),
                'position': node.get('position', {})
            }

            with open(node_dir / 'metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # è®°å½•ä½ç½®æ˜ å°„
            self.node_positions[node_id] = {
                'node_dir': str(node_dir.relative_to(self.output_dir)),
                'node_type': node_type,
                'index_in_yaml': None
            }

            return True

        except Exception as e:
            self.logger.error(f"æ‹†åˆ†é€šç”¨èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®å’Œæ˜ å°„ä¿¡æ¯"""
        try:
            # ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯
            with open(self.output_dir / 'metadata' / 'version_info.json', 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜èŠ‚ç‚¹ä½ç½®æ˜ å°„
            with open(self.output_dir / 'metadata' / 'node_positions.json', 'w', encoding='utf-8') as f:
                json.dump(self.node_positions, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åŸå§‹ç»“æ„ï¼ˆç”¨äºé‡å»ºï¼‰
            structure_info = {
                'original_file': str(self.yaml_file),
                'total_lines': len(self.raw_yaml_lines),
                'workflow_structure': {
                    'app': self.workflow_data.get('app', {}),
                    'dependencies': self.workflow_data.get('dependencies', []),
                    'kind': self.workflow_data.get('kind', ''),
                    'version': self.workflow_data.get('version', ''),
                    'workflow_meta': {
                        'conversation_variables': self.workflow_data.get('workflow', {}).get('conversation_variables', []),
                        'environment_variables': self.workflow_data.get('workflow', {}).get('environment_variables', []),
                        'features': self.workflow_data.get('workflow', {}).get('features', {}),
                        'edges': self.workflow_data.get('workflow', {}).get('graph', {}).get('edges', [])
                    }
                }
            }
            
            with open(self.output_dir / 'metadata' / 'original_structure.json', 'w', encoding='utf-8') as f:
                json.dump(structure_info, f, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºå˜æ›´æ—¥å¿—
            change_log = {
                'created': datetime.now().isoformat(),
                'operations': [
                    {
                        'type': 'split',
                        'timestamp': datetime.now().isoformat(),
                        'source_file': str(self.yaml_file),
                        'output_dir': str(self.output_dir),
                        'node_count': self.metadata['statistics']['total_nodes']
                    }
                ]
            }
            
            with open(self.output_dir / 'metadata' / 'change_log.json', 'w', encoding='utf-8') as f:
                json.dump(change_log, f, ensure_ascii=False, indent=2)

            # ç”Ÿæˆå·¥å…·åˆ†ææŠ¥å‘Š
            tools_report_path = self.output_dir / 'metadata' / 'tools_analysis_report.md'
            self.tool_manager.generate_tool_report(self.metadata['tools_analysis'], str(tools_report_path))
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")


class WorkflowRebuilder:
    """å·¥ä½œæµé‡å»ºå™¨ - å°†æ‹†åˆ†çš„æ–‡ä»¶é‡æ–°ç»„åˆæˆYAML"""
    
    def __init__(self, split_dir: str, enable_parallel: bool = True):
        self.split_dir = Path(split_dir)
        self.logger = logging.getLogger(__name__)

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.enable_parallel = enable_parallel
        self.parallel_processor = WorkflowParallelProcessor() if enable_parallel else None

        # é”™è¯¯å¤„ç†
        self.error_handler = ErrorHandler()

        # åˆå§‹åŒ–YAMLå¤„ç†å™¨
        if YAML_AVAILABLE:
            self.yaml_processor = YAML()
            self.yaml_processor.preserve_quotes = True
            self.yaml_processor.width = 4096
        else:
            self.yaml_processor = None
    
    def rebuild_workflow(self, output_file: str = None, validate_dsl: bool = True) -> bool:
        """é‡å»ºå·¥ä½œæµ

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶å
            validate_dsl: æ˜¯å¦éªŒè¯é‡å»ºåçš„DSLç»“æ„
        """
        def _rebuild_operation(output_file_param):
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
            final_output_file = output_file_param
            if final_output_file is None:
                dir_info = WorkflowVersionManager.parse_dirname_info(self.split_dir.name)
                version = dir_info.get('version', 'V1.0')
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                final_output_file = f"rebuilt_workflow_{version}_{timestamp}.yml"

            self.logger.info(f"å¼€å§‹é‡å»ºå·¥ä½œæµ: {self.split_dir} -> {final_output_file}")

            # åŠ è½½å…ƒæ•°æ®
            metadata = self._load_metadata()
            if not metadata:
                raise Exception("å…ƒæ•°æ®åŠ è½½å¤±è´¥")

            # é‡å»ºYAMLç»“æ„
            rebuilt_data = self._rebuild_yaml_structure(metadata)
            if not rebuilt_data:
                raise Exception("YAMLç»“æ„é‡å»ºå¤±è´¥")

            # éªŒè¯é‡å»ºåçš„DSLç»“æ„
            if validate_dsl:
                self.logger.info("éªŒè¯é‡å»ºåçš„DSLç»“æ„...")
                is_valid, validation_errors = DSLValidator.validate_dsl_structure(rebuilt_data)
                if not is_valid:
                    error_msg = f"é‡å»ºåçš„DSLç»“æ„éªŒè¯å¤±è´¥: {', '.join(validation_errors)}"
                    self.logger.error(error_msg)
                    raise Exception(error_msg)
                self.logger.info("é‡å»ºåçš„DSLç»“æ„éªŒè¯é€šè¿‡")

            # å†™å…¥æ–‡ä»¶
            output_path = Path(final_output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    self.yaml_processor.dump(rebuilt_data, f)
                else:
                    yaml.dump(rebuilt_data, f, default_flow_style=False, allow_unicode=True)

            self.logger.info(f"å·¥ä½œæµé‡å»ºå®Œæˆ: {output_path}")
            return True

        try:
            return self.error_handler.handle_operation(lambda: _rebuild_operation(output_file), "å·¥ä½œæµé‡å»º")
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµé‡å»ºå¤±è´¥: {e}")
            # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            error_report = self.error_handler.generate_error_report(
                self.split_dir / "logs" / "rebuild_error_report.md" if self.split_dir.exists() else None
            )
            if error_report != "æ— é”™è¯¯è®°å½•":
                self.logger.info("é‡å»ºé”™è¯¯æŠ¥å‘Šå·²ç”Ÿæˆ")
            return False
    
    def _load_metadata(self) -> Optional[Dict]:
        """åŠ è½½å…ƒæ•°æ®"""
        try:
            metadata_files = {
                'version_info': self.split_dir / 'metadata' / 'version_info.json',
                'node_positions': self.split_dir / 'metadata' / 'node_positions.json', 
                'original_structure': self.split_dir / 'metadata' / 'original_structure.json'
            }
            
            metadata = {}
            for key, file_path in metadata_files.items():
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        metadata[key] = json.load(f)
                else:
                    self.logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return None
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def _rebuild_yaml_structure(self, metadata: Dict) -> Optional[Dict]:
        """é‡å»ºYAMLç»“æ„"""
        try:
            # è·å–åŸå§‹ç»“æ„
            original_structure = metadata['original_structure']['workflow_structure']
            
            # é‡å»ºåŸºç¡€ç»“æ„
            rebuilt_data = {
                'app': original_structure['app'],
                'dependencies': original_structure['dependencies'],
                'kind': original_structure['kind'],
                'version': original_structure['version'],
                'workflow': {
                    'conversation_variables': original_structure['workflow_meta']['conversation_variables'],
                    'environment_variables': original_structure['workflow_meta']['environment_variables'],
                    'features': original_structure['workflow_meta']['features'],
                    'graph': {
                        'edges': original_structure['workflow_meta']['edges'],
                        'nodes': []
                    }
                }
            }
            
            # é‡å»ºèŠ‚ç‚¹
            nodes = self._rebuild_nodes(metadata['node_positions'])
            if nodes is None:
                return None
            
            rebuilt_data['workflow']['graph']['nodes'] = nodes
            
            return rebuilt_data
            
        except Exception as e:
            self.logger.error(f"é‡å»ºYAMLç»“æ„å¤±è´¥: {e}")
            return None
    
    def _rebuild_nodes(self, node_positions: Dict) -> Optional[List]:
        """é‡å»ºèŠ‚ç‚¹åˆ—è¡¨ - æ”¯æŒå¹¶è¡Œå¤„ç†"""
        try:
            nodes = []

            # æŒ‰èŠ‚ç‚¹ç±»å‹å’Œç´¢å¼•æ’åº
            sorted_nodes = sorted(node_positions.items(),
                                key=lambda x: (x[1]['node_type'], x[1]['node_dir']))

            if self.enable_parallel and len(sorted_nodes) > 3:  # åªæœ‰åœ¨èŠ‚ç‚¹è¾ƒå¤šæ—¶æ‰ä½¿ç”¨å¹¶è¡Œå¤„ç†
                self.logger.info("ä½¿ç”¨å¹¶è¡Œå¤„ç†é‡å»ºèŠ‚ç‚¹...")

                # å‡†å¤‡èŠ‚ç‚¹é‡å»ºä»»åŠ¡
                def rebuild_node_task(node_info):
                    node_id, position_info = node_info
                    node_dir = self.split_dir / position_info['node_dir']
                    return self._rebuild_single_node(node_id, node_dir, position_info['node_type'])

                # å¹¶è¡Œé‡å»ºèŠ‚ç‚¹
                results = self.parallel_processor.process_nodes_parallel(
                    [{'node_id': node_id, 'position_info': position_info}
                     for node_id, position_info in sorted_nodes],
                    lambda task, **kwargs: rebuild_node_task((task['node_id'], task['position_info']))
                )

                # æ”¶é›†ç»“æœï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
                for node_id, position_info in sorted_nodes:
                    node = results.get(node_id)
                    if node:
                        nodes.append(node)
                    else:
                        self.logger.warning(f"é‡å»ºèŠ‚ç‚¹å¤±è´¥: {node_id}")

            else:
                # ä¸²è¡Œé‡å»º
                self.logger.info("ä½¿ç”¨ä¸²è¡Œå¤„ç†é‡å»ºèŠ‚ç‚¹...")
                for node_id, position_info in sorted_nodes:
                    node_dir = self.split_dir / position_info['node_dir']

                    # é‡å»ºå•ä¸ªèŠ‚ç‚¹
                    node = self._rebuild_single_node(node_id, node_dir, position_info['node_type'])
                    if node:
                        nodes.append(node)
                    else:
                        self.logger.warning(f"é‡å»ºèŠ‚ç‚¹å¤±è´¥: {node_id}")

            return nodes

        except Exception as e:
            self.logger.error(f"é‡å»ºèŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: {e}")
            return None
    
    def _rebuild_single_node(self, node_id: str, node_dir: Path, node_type: str) -> Optional[Dict]:
        """é‡å»ºå•ä¸ªèŠ‚ç‚¹"""
        try:
            # åŠ è½½èŠ‚ç‚¹å…ƒæ•°æ®
            with open(node_dir / 'metadata.json', 'r', encoding='utf-8') as f:
                node_metadata = json.load(f)
            
            # åŠ è½½åŸºç¡€é…ç½®
            with open(node_dir / 'config.yaml', 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    node_config = self.yaml_processor.load(f)
                else:
                    node_config = yaml.safe_load(f)
            
            # æ ¹æ®èŠ‚ç‚¹ç±»å‹é‡å»ºç‰¹å®šå†…å®¹ - æ”¯æŒæ›´å¤šDifyèŠ‚ç‚¹ç±»å‹
            if node_type == 'llm':
                return self._rebuild_llm_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'code':
                return self._rebuild_code_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'agent':
                return self._rebuild_agent_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'tool':
                return self._rebuild_tool_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'http_request':
                return self._rebuild_http_request_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'if_else':
                return self._rebuild_if_else_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'iteration':
                return self._rebuild_iteration_node(node_id, node_dir, node_metadata, node_config)
            elif node_type == 'template_transform':
                return self._rebuild_template_transform_node(node_id, node_dir, node_metadata, node_config)
            elif node_type in ['start', 'end']:
                return self._rebuild_flow_control_node(node_id, node_metadata, node_config)
            elif node_type.startswith('trigger_'):
                return self._rebuild_trigger_node(node_id, node_dir, node_metadata, node_config)
            else:
                return self._rebuild_generic_node(node_id, node_metadata, node_config)
                
        except Exception as e:
            self.logger.error(f"é‡å»ºå•ä¸ªèŠ‚ç‚¹å¤±è´¥ {node_id}: {e}")
            return None
    
    def _rebuild_llm_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºLLMèŠ‚ç‚¹"""
        # é‡å»ºæç¤ºè¯æ¨¡æ¿
        prompt_template = []
        
        for prompt_file in metadata.get('prompt_files', []):
            prompt_path = node_dir / prompt_file
            if prompt_path.exists():
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    prompt_text = f.read()
                
                # ä»æ–‡ä»¶åæå–è§’è‰²ä¿¡æ¯
                role = 'user'
                if 'system_prompt' in prompt_file:
                    role = 'system'
                elif 'user_prompt' in prompt_file:
                    role = 'user'
                
                prompt_template.append({
                    'edition_type': 'basic',
                    'id': f"{node_id}_prompt_{len(prompt_template)}",
                    'role': role,
                    'text': prompt_text
                })
        
        # åˆå¹¶é…ç½®
        config['prompt_template'] = prompt_template
        
        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 115),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }
    
    def _rebuild_code_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºä»£ç èŠ‚ç‚¹"""
        # é‡å»ºä»£ç å†…å®¹
        code_file = metadata.get('code_file', 'code.py')
        code_path = node_dir / code_file
        
        if code_path.exists():
            with open(code_path, 'r', encoding='utf-8') as f:
                code_content = f.read()
            config['code'] = code_content
        
        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 52),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }
    
    def _rebuild_agent_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºAgentèŠ‚ç‚¹"""
        # é‡å»ºæç¤ºè¯æ¨¡æ¿
        prompt_template = []

        for prompt_file in metadata.get('prompt_files', []):
            prompt_path = node_dir / prompt_file
            if prompt_path.exists():
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    prompt_text = f.read()

                # ä»æ–‡ä»¶åæå–è§’è‰²ä¿¡æ¯
                role = 'user'
                if 'system_prompt' in prompt_file:
                    role = 'system'
                elif 'user_prompt' in prompt_file:
                    role = 'user'

                prompt_template.append({
                    'edition_type': 'basic',
                    'id': f"{node_id}_prompt_{len(prompt_template)}",
                    'role': role,
                    'text': prompt_text
                })

        # åŠ è½½Agentå‚æ•°
        agent_parameters = {}
        agent_params_file = node_dir / 'agent_parameters.yaml'
        if agent_params_file.exists():
            with open(agent_params_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    agent_parameters = self.yaml_processor.load(f)
                else:
                    agent_parameters = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config['prompt_template'] = prompt_template
        config['agent_parameters'] = agent_parameters

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 125),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_tool_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºå·¥å…·èŠ‚ç‚¹"""
        # åŠ è½½å·¥å…·é…ç½®
        tool_config = {}
        tool_config_file = node_dir / 'tool_config.yaml'
        if tool_config_file.exists():
            with open(tool_config_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    tool_config = self.yaml_processor.load(f)
                else:
                    tool_config = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config.update(tool_config)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 115),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_http_request_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºHTTPè¯·æ±‚èŠ‚ç‚¹"""
        # åŠ è½½HTTPé…ç½®
        http_config = {}
        http_config_file = node_dir / 'http_config.yaml'
        if http_config_file.exists():
            with open(http_config_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    http_config = self.yaml_processor.load(f)
                else:
                    http_config = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config.update(http_config)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 115),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_if_else_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºæ¡ä»¶åˆ†æ”¯èŠ‚ç‚¹"""
        # åŠ è½½é€»è¾‘é…ç½®
        logic_config = {}
        logic_config_file = node_dir / 'logic_config.yaml'
        if logic_config_file.exists():
            with open(logic_config_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    logic_config = self.yaml_processor.load(f)
                else:
                    logic_config = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config.update(logic_config)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 95),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_iteration_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºè¿­ä»£èŠ‚ç‚¹"""
        # åŠ è½½è¿­ä»£é…ç½®
        iteration_config = {}
        iteration_config_file = node_dir / 'iteration_config.yaml'
        if iteration_config_file.exists():
            with open(iteration_config_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    iteration_config = self.yaml_processor.load(f)
                else:
                    iteration_config = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config.update(iteration_config)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 95),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_template_transform_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºæ¨¡æ¿è½¬æ¢èŠ‚ç‚¹"""
        # åŠ è½½æ¨¡æ¿å†…å®¹
        template_file = node_dir / 'template.txt'
        if template_file.exists():
            with open(template_file, 'r', encoding='utf-8') as f:
                config['template'] = f.read()

        # åŠ è½½å˜é‡é€‰æ‹©å™¨é…ç½®
        variable_selectors_file = node_dir / 'variable_selectors.yaml'
        if variable_selectors_file.exists():
            with open(variable_selectors_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    config['variable_selectors'] = self.yaml_processor.load(f)
                else:
                    config['variable_selectors'] = yaml.safe_load(f)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 95),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_flow_control_node(self, node_id: str, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºæµç¨‹æ§åˆ¶èŠ‚ç‚¹"""
        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 54),
            'width': metadata.get('width', 120),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_trigger_node(self, node_id: str, node_dir: Path, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºè§¦å‘å™¨èŠ‚ç‚¹"""
        # åŠ è½½è§¦å‘å™¨é…ç½®
        trigger_config = {}
        trigger_config_file = node_dir / 'trigger_config.yaml'
        if trigger_config_file.exists():
            with open(trigger_config_file, 'r', encoding='utf-8') as f:
                if YAML_AVAILABLE:
                    trigger_config = self.yaml_processor.load(f)
                else:
                    trigger_config = yaml.safe_load(f)

        # åˆå¹¶é…ç½®
        config.update(trigger_config)

        # æ„å»ºå®Œæ•´èŠ‚ç‚¹
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 54),
            'width': metadata.get('width', 120),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }

    def _rebuild_generic_node(self, node_id: str, metadata: Dict, config: Dict) -> Dict:
        """é‡å»ºé€šç”¨èŠ‚ç‚¹"""
        return {
            'data': config,
            'id': node_id,
            'position': metadata.get('position', {}),
            'height': metadata.get('height', 115),
            'width': metadata.get('width', 242),
            'selected': False,
            'sourcePosition': 'right',
            'targetPosition': 'left',
            'type': 'custom'
        }


class WorkflowComparator:
    """å·¥ä½œæµå¯¹æ¯”å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def compare_workflows(self, file1: str, file2: str, output_file: str = None) -> bool:
        """å¯¹æ¯”ä¸¤ä¸ªå·¥ä½œæµæ–‡ä»¶"""
        try:
            self.logger.info(f"å¼€å§‹å¯¹æ¯”å·¥ä½œæµ: {file1} vs {file2}")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file1, 'r', encoding='utf-8') as f:
                content1 = f.readlines()
            
            with open(file2, 'r', encoding='utf-8') as f:
                content2 = f.readlines()
            
            # ç”Ÿæˆå·®å¼‚
            differ = difflib.unified_diff(
                content1, content2,
                fromfile=file1,
                tofile=file2,
                lineterm='',
                n=3
            )
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"workflow_diff_{timestamp}.html"
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            self._generate_html_diff_report(file1, file2, content1, content2, output_file)
            
            # ç”Ÿæˆæ–‡æœ¬å·®å¼‚
            text_diff_file = output_file.replace('.html', '.txt')
            with open(text_diff_file, 'w', encoding='utf-8') as f:
                f.write(f"å·¥ä½œæµå¯¹æ¯”æŠ¥å‘Š\n")
                f.write(f"æ–‡ä»¶1: {file1}\n")
                f.write(f"æ–‡ä»¶2: {file2}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n")
                f.write("=" * 80 + "\n\n")
                
                differ = difflib.unified_diff(
                    content1, content2,
                    fromfile=file1,
                    tofile=file2,
                    lineterm='',
                    n=3
                )
                
                for line in differ:
                    f.write(line + '\n')
            
            self.logger.info(f"å¯¹æ¯”æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"å¯¹æ¯”å·¥ä½œæµå¤±è´¥: {e}")
            return False
    
    def _generate_html_diff_report(self, file1: str, file2: str, content1: List[str], 
                                 content2: List[str], output_file: str):
        """ç”ŸæˆHTMLå·®å¼‚æŠ¥å‘Š"""
        # ç”ŸæˆHTMLå·®å¼‚è§†å›¾
        differ = difflib.HtmlDiff()
        html_diff = differ.make_file(
            content1, content2,
            fromdesc=f"åŸå§‹æ–‡ä»¶: {file1}",
            todesc=f"é‡å»ºæ–‡ä»¶: {file2}",
            context=True,
            numlines=3
        )
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_diff)


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("Dify Workflow Manager v2.0 - å¢å¼ºç‰ˆ")
        print("=" * 50)
        print("æ”¯æŒDify 40+ç§èŠ‚ç‚¹ç±»å‹ï¼ŒDSLéªŒè¯ï¼Œå¹¶è¡Œå¤„ç†")
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python dify_workflow_manager_v2.py split <yaml_file> [--no-parallel] [--no-validate]")
        print("  python dify_workflow_manager_v2.py rebuild <split_dir> [--no-parallel] [--no-validate]")
        print("  python dify_workflow_manager_v2.py compare <file1> <file2>")
        print("  python dify_workflow_manager_v2.py validate <yaml_file>")
        print("  python dify_workflow_manager_v2.py analyze-tools <yaml_file>")
        print("  python dify_workflow_manager_v2.py check-version <yaml_file>")
        print("  python dify_workflow_manager_v2.py migrate <yaml_file> <target_version> [output_file]")
        print()
        print("é€‰é¡¹:")
        print("  --no-parallel    ç¦ç”¨å¹¶è¡Œå¤„ç†")
        print("  --no-validate    ç¦ç”¨DSLéªŒè¯")
        print()
        print("ç¤ºä¾‹:")
        print("  python dify_workflow_manager_v2.py split \"AI Code Review-V4.1.yml\"")
        print("  python dify_workflow_manager_v2.py rebuild \"parsed_workflow_V4.1_20240115_143022\" --no-parallel")
        print("  python dify_workflow_manager_v2.py validate \"workflow.yml\"")
        return 1
    
    command = sys.argv[1].lower()

    # è§£æé€‰é¡¹
    enable_parallel = True
    validate_dsl = True

    args = sys.argv[2:]
    filtered_args = []

    for arg in args:
        if arg == '--no-parallel':
            enable_parallel = False
        elif arg == '--no-validate':
            validate_dsl = False
        else:
            filtered_args.append(arg)

    if command == 'split':
        if len(filtered_args) < 1:
            print("é”™è¯¯: è¯·æä¾›YAMLæ–‡ä»¶è·¯å¾„")
            return 1

        yaml_file = filtered_args[0]
        output_dir = filtered_args[1] if len(filtered_args) > 1 else None

        splitter = WorkflowSplitter(yaml_file, output_dir, enable_parallel=enable_parallel)
        success = splitter.split_workflow(validate_dsl=validate_dsl)

        if success:
            parallel_info = " (å¹¶è¡Œå¤„ç†)" if enable_parallel else " (ä¸²è¡Œå¤„ç†)"
            print(f"[SUCCESS] å·¥ä½œæµæ‹†åˆ†æˆåŠŸ{parallel_info}!")
            print(f"[OUTPUT] è¾“å‡ºç›®å½•: {splitter.output_dir}")
        else:
            print("[ERROR] å·¥ä½œæµæ‹†åˆ†å¤±è´¥!")
            return 1
    
    elif command == 'rebuild':
        if len(filtered_args) < 1:
            print("é”™è¯¯: è¯·æä¾›æ‹†åˆ†ç›®å½•è·¯å¾„")
            return 1

        split_dir = filtered_args[0]
        output_file = filtered_args[1] if len(filtered_args) > 1 else None

        rebuilder = WorkflowRebuilder(split_dir, enable_parallel=enable_parallel)
        success = rebuilder.rebuild_workflow(output_file, validate_dsl=validate_dsl)

        if success:
            parallel_info = " (å¹¶è¡Œå¤„ç†)" if enable_parallel else " (ä¸²è¡Œå¤„ç†)"
            print(f"[SUCCESS] å·¥ä½œæµé‡å»ºæˆåŠŸ{parallel_info}!")
        else:
            print("[ERROR] å·¥ä½œæµé‡å»ºå¤±è´¥!")
            return 1
    
    elif command == 'validate':
        if len(filtered_args) < 1:
            print("é”™è¯¯: è¯·æä¾›YAMLæ–‡ä»¶è·¯å¾„")
            return 1

        yaml_file = filtered_args[0]

        # åŠ è½½å¹¶éªŒè¯DSL
        try:
            if YAML_AVAILABLE:
                yaml_processor = YAML()
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml_processor.load(f)
            else:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml.safe_load(f)

            print(f"æ­£åœ¨éªŒè¯DSLæ–‡ä»¶: {yaml_file}")
            is_valid, validation_errors = DSLValidator.validate_dsl_structure(workflow_data)

            if is_valid:
                print("[SUCCESS] DSLç»“æ„éªŒè¯é€šè¿‡!")
                print("å·¥ä½œæµç»“æ„å®Œæ•´ä¸”ç¬¦åˆDifyè§„èŒƒ")
            else:
                print("[ERROR] DSLç»“æ„éªŒè¯å¤±è´¥:")
                for error in validation_errors:
                    print(f"  - {error}")
                return 1

        except Exception as e:
            print(f"[ERROR] DSLéªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return 1

    elif command == 'analyze-tools':
        if len(filtered_args) < 1:
            print("é”™è¯¯: è¯·æä¾›YAMLæ–‡ä»¶è·¯å¾„")
            return 1

        yaml_file = filtered_args[0]
        output_file = filtered_args[1] if len(filtered_args) > 1 else None

        # åŠ è½½å¹¶åˆ†æå·¥å…·
        try:
            if YAML_AVAILABLE:
                yaml_processor = YAML()
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml_processor.load(f)
            else:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml.safe_load(f)

            print(f"æ­£åœ¨åˆ†æå·¥ä½œæµå·¥å…·: {yaml_file}")

            tool_manager = ToolManager()
            tools_analysis = tool_manager.analyze_tools_in_workflow(workflow_data)

            # ç”ŸæˆæŠ¥å‘Š
            if output_file:
                report_path = output_file
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                report_path = f"tools_analysis_{timestamp}.md"

            report = tool_manager.generate_tool_report(tools_analysis, report_path)
            print("[SUCCESS] å·¥å…·åˆ†æå®Œæˆ!")
            print(f"[REPORT] æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

            # åœ¨æ§åˆ¶å°æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
            stats = tools_analysis['statistics']
            print(f"[STATS] å·¥å…·ç»Ÿè®¡: æ€»è®¡ {stats['total_tools']} ä¸ªå·¥å…·ï¼Œ{stats['enabled_tools']} ä¸ªå¯ç”¨")

        except Exception as e:
            print(f"[ERROR] å·¥å…·åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return 1

    elif command == 'check-version':
        if len(filtered_args) < 1:
            print("é”™è¯¯: è¯·æä¾›YAMLæ–‡ä»¶è·¯å¾„")
            return 1

        yaml_file = filtered_args[0]

        try:
            if YAML_AVAILABLE:
                yaml_processor = YAML()
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml_processor.load(f)
            else:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml.safe_load(f)

            print(f"æ­£åœ¨æ£€æŸ¥å·¥ä½œæµç‰ˆæœ¬: {yaml_file}")

            # ç”Ÿæˆç‰ˆæœ¬æŠ¥å‘Š
            version_report = WorkflowVersionManager.generate_version_report(workflow_data)

            # æ˜¾ç¤ºæŠ¥å‘Š
            print(version_report)

        except Exception as e:
            print(f"[ERROR] ç‰ˆæœ¬æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return 1

    elif command == 'migrate':
        if len(filtered_args) < 2:
            print("é”™è¯¯: è¯·æä¾›YAMLæ–‡ä»¶è·¯å¾„å’Œç›®æ ‡ç‰ˆæœ¬")
            return 1

        yaml_file = filtered_args[0]
        target_version = filtered_args[1]
        output_file = filtered_args[2] if len(filtered_args) > 2 else None

        try:
            if YAML_AVAILABLE:
                yaml_processor = YAML()
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml_processor.load(f)
            else:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    workflow_data = yaml.safe_load(f)

            print(f"æ­£åœ¨è¿ç§»å·¥ä½œæµç‰ˆæœ¬: {yaml_file}")
            print(f"ç›®æ ‡ç‰ˆæœ¬: {target_version}")

            # æ‰§è¡Œè¿ç§»
            success, migrated_data, migration_log = WorkflowVersionManager.migrate_workflow(
                workflow_data, target_version
            )

            if success:
                print("[SUCCESS] ç‰ˆæœ¬è¿ç§»æˆåŠŸ!")

                # æ˜¾ç¤ºè¿ç§»æ—¥å¿—
                for log_entry in migration_log:
                    print(f"  - {log_entry}")

                # ä¿å­˜è¿ç§»åçš„æ–‡ä»¶
                if output_file:
                    output_path = output_file
                else:
                    base_name = Path(yaml_file).stem
                    output_path = f"{base_name}_migrated_{target_version}.yml"

                with open(output_path, 'w', encoding='utf-8') as f:
                    if YAML_AVAILABLE:
                        yaml_processor.dump(migrated_data, f)
                    else:
                        yaml.dump(migrated_data, f, default_flow_style=False, allow_unicode=True)

                print(f"[OUTPUT] è¿ç§»åçš„æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")

            else:
                print("[ERROR] ç‰ˆæœ¬è¿ç§»å¤±è´¥:")
                for error in migration_log:
                    print(f"  - {error}")
                return 1

        except Exception as e:
            print(f"[ERROR] ç‰ˆæœ¬è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return 1

    elif command == 'compare':
        if len(filtered_args) < 2:
            print("é”™è¯¯: è¯·æä¾›ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„")
            return 1

        file1 = filtered_args[0]
        file2 = filtered_args[1]
        output_file = filtered_args[2] if len(filtered_args) > 2 else None

        comparator = WorkflowComparator()
        success = comparator.compare_workflows(file1, file2, output_file)

        if success:
            print("[SUCCESS] å·¥ä½œæµå¯¹æ¯”å®Œæˆ!")
        else:
            print("[ERROR] å·¥ä½œæµå¯¹æ¯”å¤±è´¥!")
            return 1
    
    else:
        print(f"é”™è¯¯: æœªçŸ¥å‘½ä»¤ '{command}'")
        print("æ”¯æŒçš„å‘½ä»¤: split, rebuild, compare")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())






